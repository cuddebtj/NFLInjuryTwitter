{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9e63b1-15a3-45cf-9250-1b0582e999a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66be0d4-ea5a-4489-9894-d7da235b6d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# login payload information pulled from a .env file\n",
    "stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "stat_user_name = os.environ.get('statheadusername')\n",
    "stat_password = os.environ.get('statheadpassword')\n",
    "stat_payload = {\n",
    "    'username': stat_user_name,\n",
    "    'password': stat_password\n",
    "}\n",
    "# url for team data\n",
    "team_data_url = {'1': 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=points&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=E&game_num_min=0&year_min={Season}&cstat[1]=all_td_team&ccomp[1]=gt&cval[1]=0&cstat[2]=third_down_att&ccomp[2]=gt&cval[2]=0&cstat[3]=vegas_line&ccomp[3]=gt&cval[3]=-50&cstat[4]=penalties&ccomp[4]=gt&cval[4]=0&cstat[5]=rush_att&ccomp[5]=gt&cval[5]=0&cstat[6]=tot_yds&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=pass_cmp&ccomp[9]=gt&cval[9]=0&offset={page}', \n",
    "                 '2': 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=all_td_opp&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={Season}&cstat[1]=tot_yds_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_yds_diff&ccomp[2]=gt&cval[2]=-500&cstat[3]=score_diff_thru_1&ccomp[3]=gt&cval[3]=-500&cstat[4]=rush_att_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=kick_ret_td_tgl&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp_opp&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down_opp&ccomp[7]=gt&cval[7]=0&cstat[8]=score_diff_1_qtr&ccomp[8]=gt&cval[8]=-500&cstat[9]=third_down_att_opp&ccomp[9]=gt&cval[9]=0&offset={page}'}\n",
    "# injury report url\n",
    "injury_url = 'https://www.pro-football-reference.com/teams/{team}/{Season}_injuries.htm'\n",
    "# player database url\n",
    "player_url = \"https://www.pro-football-reference.com/players/{abcd}/\"\n",
    "# page to start scrape at\n",
    "page = 0\n",
    "# season to scrape\n",
    "s = 2020\n",
    "# list of team abbreviations from pro-football-reference for url purposes\n",
    "teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', \n",
    "         'dal', 'den', 'det', 'gnb', 'htx', 'clt', 'jax', 'kan',\n",
    "         'sdg', 'ram', 'mia', 'min', 'nor', 'nwe', 'nyg', 'nyj',\n",
    "         'rai', 'phi', 'pit', 'sea', 'sfo', 'tam', 'oti', 'was']\n",
    "abcd = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f90430-a59d-45ef-b9ab-5d051f38e201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create player database\n",
    "start = time.time()\n",
    "players = []\n",
    "for c in range(0,len(abcd)):\n",
    "    res = requests.get(player_url.format(abcd=abcd[c]))\n",
    "    soup = BeautifulSoup(res.content, \n",
    "                         'html.parser')\n",
    "    data = soup.find_all('p')\n",
    "    for i in data:\n",
    "        a = i.find('a')['href']\n",
    "        pl = i.get_text()[: (i.get_text().find(\"(\")-1)]\n",
    "        po = i.get_text()[(i.get_text().find(\"(\")+1): i.get_text().find(\")\")]\n",
    "        fy = i.get_text()[(i.get_text().rfind(\"-\")-4): (i.get_text().rfind(\"-\"))]\n",
    "        ly = i.get_text()[(i.get_text().rfind(\"-\")+1): (i.get_text().rfind(\"-\")+5)]\n",
    "        p = {\"Address\": a[: -4], \n",
    "             \"Player\": pl, \n",
    "             \"Position\": po, \n",
    "             \"First_Year\": fy, \n",
    "             \"Last_Year\": ly}\n",
    "        players.append(p)\n",
    "players_df = pd.DataFrame().from_dict(players)\n",
    "players_df = players_df[players_df.Last_Year != \"Ever\"]\n",
    "players_df['Player'] = players_df['Player'].str.replace('+', '')\n",
    "players_df['Last_Year'] = players_df.Last_Year.astype('int32')\n",
    "players_df.to_csv('../data/database-players.csv',\n",
    "                  index=False)\n",
    "end = time.time()\n",
    "print(f'Done creating player database.', \n",
    "      f'Time to complete: {(end-start)/6} minutes', \n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7749725-73c7-4e82-a423-047522563b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scrape all team data (requires stathead subscription)\n",
    "start = time.time()\n",
    "with requests.Session() as session:\n",
    "    sess = session.post(stat_login_url, \n",
    "                        data=stat_payload)\n",
    "    try:\n",
    "        for url in team_data_url:\n",
    "            while page < 100000:          \n",
    "                website = session.get(team_data_url[url].format(Season=s, \n",
    "                                                                page=page)).text\n",
    "                soup = BeautifulSoup(website, \n",
    "                                     'html')\n",
    "                table = soup.find('table', \n",
    "                                  attrs={'class': 'sortable', \n",
    "                                         'id': 'results'})\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "                final_data = []\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "                df = pd.DataFrame(final_data[1:], \n",
    "                                  columns=table_headers[12:])\n",
    "                df.to_csv(f'../data/raw-data/nfl-team-data-{s}-{url}-raw.csv', \n",
    "                          mode='a', \n",
    "                          index=False)\n",
    "                page += 100\n",
    "    except:\n",
    "        end = time.time()\n",
    "        print(f'Done: Team {s}, {url}, {page}', \n",
    "              f'Time to complete: {end-start}', \n",
    "              sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca0053-d6ff-46f7-b894-8a8017c36d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scrape weekly nfl injury reports\n",
    "start = time.time()\n",
    "injury_final_df = pd.DataFrame()\n",
    "for team in teams:\n",
    "    res = requests.get(injury_url.format(Season=s, \n",
    "                                         team=team))\n",
    "    soup = BeautifulSoup(res.content, \n",
    "                         'lxml')\n",
    "    table = soup.find('table', \n",
    "                      attrs={'class': 'sortable', \n",
    "                             'id': 'team_injuries'})\n",
    "    table_rows = table.find_all('tr')\n",
    "    final_data = []\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all(['th','td'])\n",
    "        row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "        #p_add = [tr['player-address'] if tr.has_attr('a') else tr.text for tr in td]\n",
    "        final_data.append(row)\n",
    "    df_data = final_data[1:]\n",
    "    data_body = [[df_data[j][i] for j in range(len(df_data))] for i in range(len(df_data[0]))]\n",
    "    df = pd.DataFrame(data_body,\n",
    "                      final_data[0]).T\n",
    "    df.insert(loc=1,\n",
    "              column='Team',\n",
    "              value=team)\n",
    "    df.insert(loc=2,\n",
    "              column='Season',\n",
    "              value=s)\n",
    "    injury_final_df = pd.concat([injury_final_df, \n",
    "                                 df])\n",
    "injury_final_df.rename(columns={'PlayerÂ ':'Player'},\n",
    "                       inplace=True)\n",
    "injury_final_df.to_csv(f'../data/raw-data/nfl-injury-report-{s}-raw.csv',\n",
    "                       index=False)\n",
    "end = time.time()\n",
    "print(f'Done: Injury Reports {s}',\n",
    "      f'Time to complete: {end-start}',\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01ddab27-6e66-430a-9681-f91983ef2ae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data-raw/player-weekly-stats-2020-raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3e9dd1012aca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m player_stats_df.to_csv(f'../data-raw/player-weekly-stats-{s}-raw.csv',\n\u001b[0m\u001b[0;32m     19\u001b[0m                        index=False)\n\u001b[0;32m     20\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data-raw/player-weekly-stats-2020-raw.csv'"
     ]
    }
   ],
   "source": [
    "# scarpe player weekly stats\n",
    "start = time.time()\n",
    "players = pd.read_csv('../data/database-players.csv')\n",
    "player_stats_df = pd.DataFrame()\n",
    "for p_add in players['Address'][:15]:\n",
    "    try:\n",
    "        df = pd.read_html(f'https://www.pro-football-reference.com{p_add}/gamelog/{s}', \n",
    "                          header=[0,1], \n",
    "                          attrs={'id': 'stats'})\n",
    "        df = df[0]\n",
    "        df = df.head(-1)\n",
    "        df['Player_Address'] = p_add\n",
    "        df['Season'] = s\n",
    "        player_stats_df = pd.concat([df, \n",
    "                                     player_stats_df])\n",
    "    except:\n",
    "        continue\n",
    "player_stats_df.to_csv(f'..data/data-raw/player-weekly-stats-{s}-raw.csv',\n",
    "                       index=False)\n",
    "end = time.time()\n",
    "print(f'Done: Player stats scrape (including snap counts) for {s} season',\n",
    "      f'{(end-start)/60}: minutes to complete.', \n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d493a9-f5c5-497f-9b28-12df705255eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
