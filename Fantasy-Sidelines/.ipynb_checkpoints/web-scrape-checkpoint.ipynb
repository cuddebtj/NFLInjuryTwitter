{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9e63b1-15a3-45cf-9250-1b0582e999a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f90430-a59d-45ef-b9ab-5d051f38e201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scrape a player database\n",
    "def player_database():\n",
    "    start = time.time()\n",
    "    player_url = \"https://www.pro-football-reference.com/players/{abcd}/\"\n",
    "    abcd = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    players = []\n",
    "    for c in range(0,len(abcd)):\n",
    "        res = requests.get(player_url.format(abcd=abcd[c]))\n",
    "        soup = BeautifulSoup(res.content, \n",
    "                             'html.parser')\n",
    "        data = soup.find_all('p')\n",
    "        try:\n",
    "            for i in data:\n",
    "                a = i.find('a')['href']\n",
    "                pl = i.get_text()[: (i.get_text().find(\"(\")-1)]\n",
    "                po = i.get_text()[(i.get_text().find(\"(\")+1): i.get_text().find(\")\")]\n",
    "                fy = i.get_text()[(i.get_text().rfind(\"-\")-4): (i.get_text().rfind(\"-\"))]\n",
    "                ly = i.get_text()[(i.get_text().rfind(\"-\")+1): (i.get_text().rfind(\"-\")+5)]\n",
    "                p = {\"Address\": a[: -4], \n",
    "                     \"Player\": pl, \n",
    "                     \"Position\": po, \n",
    "                     \"First_Year\": fy, \n",
    "                     \"Last_Year\": ly}\n",
    "                players.append(p)\n",
    "        except:\n",
    "            continue\n",
    "    players_df = pd.DataFrame().from_dict(players)\n",
    "    players_df = players_df[players_df.Last_Year != \"Ever\"]\n",
    "    players_df['Player'] = players_df['Player'].str.replace('+', '')\n",
    "    players_df['Last_Year'] = players_df.Last_Year.astype('int32')\n",
    "    players_df.to_csv('../data/database-players.csv',\n",
    "                      index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done creating player database.', \n",
    "          f'Time to complete: {(end-start)/60} minutes', \n",
    "          sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7749725-73c7-4e82-a423-047522563b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scrape all team data (requires stathead subscription)\n",
    "def team_data(s):\n",
    "    start = time.time()\n",
    "    page = 0\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    team_data_url = \"https://stathead.com/football/tgl_finder.cgi?request=1&match=game&order_by=pass_td&year_min={Season}&year_max={Season}&game_type=R&ccomp%5B1%5D=gt&cval%5B1%5D=-500&cstat%5B1%5D=first_down&ccomp%5B2%5D=gt&cval%5B2%5D=-500&cstat%5B2%5D=rush_att&ccomp%5B3%5D=gt&cval%5B3%5D=-500&cstat%5B3%5D=third_down_att&ccomp%5B4%5D=gt&cval%5B4%5D=-500&cstat%5B4%5D=punt&ccomp%5B5%5D=gt&cval%5B5%5D=-500&cstat%5B5%5D=pass_cmp&ccomp%5B6%5D=gt&cval%5B6%5D=-500&cstat%5B6%5D=all_td_team&ccomp%5B7%5D=gt&cval%5B7%5D=-500&cstat%5B7%5D=points&ccomp%5B8%5D=gt&cval%5B8%5D=-500&cstat%5B8%5D=tot_yds&ccomp%5B9%5D=gt&cval%5B9%5D=-500&cstat%5B9%5D=kick_ret_td_tgl&ccomp%5B10%5D=gt&cval%5B10%5D=-500&cstat%5B10%5D=penalties&game_num_min=0&game_num_max=99&week_num_min=0&week_num_max=99&temperature_gtlt=lt&offset={page}\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    with requests.Session() as session:\n",
    "        sess = session.post(stat_login_url, \n",
    "                            data=stat_payload)\n",
    "        try:\n",
    "            while page < 100000:          \n",
    "                website = session.get(team_data_url.format(Season=s, \n",
    "                                                                page=page)).text\n",
    "                soup = BeautifulSoup(website, \n",
    "                                     'html')\n",
    "                table = soup.find('table', \n",
    "                                  attrs={'id': 'results'})\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "                final_data = []\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "                df = pd.DataFrame(final_data[1:], \n",
    "                                  columns=table_headers[12:])\n",
    "                df.to_csv(f'../data/raw-data/nfl-team-data-{s}-raw.csv', \n",
    "                          mode='a', \n",
    "                          index=False)\n",
    "                page += 100\n",
    "        except:\n",
    "            end = time.time()\n",
    "            print(f'Done: Team {s}, {page}',\n",
    "                  f'Time to complete: {end-start}', \n",
    "                  sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca0053-d6ff-46f7-b894-8a8017c36d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scrape weekly nfl injury reports\n",
    "# season to scrape\n",
    "def injuy_reports(s):\n",
    "    start = time.time()\n",
    "    injury_url = 'https://www.pro-football-reference.com/teams/{team}/{Season}_injuries.htm'\n",
    "    teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', \n",
    "         'dal', 'den', 'det', 'gnb', 'htx', 'clt', 'jax', 'kan',\n",
    "         'sdg', 'ram', 'mia', 'min', 'nor', 'nwe', 'nyg', 'nyj',\n",
    "         'rai', 'phi', 'pit', 'sea', 'sfo', 'tam', 'oti', 'was']\n",
    "    injury_final_df = pd.DataFrame()\n",
    "    for team in teams:\n",
    "        res = requests.get(injury_url.format(Season=s, \n",
    "                                             team=team))\n",
    "        soup = BeautifulSoup(res.content, \n",
    "                             'lxml')\n",
    "        table = soup.find('table', \n",
    "                          attrs={'class': 'sortable', \n",
    "                                 'id': 'team_injuries'})\n",
    "        table_rows = table.find_all('tr')\n",
    "        final_data = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all(['th','td'])\n",
    "            row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "            p_add = tr.find('a')['href'][:-4]\n",
    "            row.insert(1, p_add)\n",
    "            final_data.append(row)\n",
    "        df_data = final_data[1:]\n",
    "        data_body = [[df_data[j][i] for j in range(len(df_data))] for i in range(len(df_data[0]))]\n",
    "        df = pd.DataFrame(data_body,\n",
    "                          final_data[0]).T\n",
    "        df.insert(loc=1,\n",
    "                  column='Team',\n",
    "                  value=team)\n",
    "        df.insert(loc=2,\n",
    "                  column='Season',\n",
    "                  value=s)\n",
    "        injury_final_df = pd.concat([injury_final_df, \n",
    "                                     df])\n",
    "    injury_final_df.rename(columns= {'Player\\xa0': 'Player'},\n",
    "                           inplace=True)\n",
    "    injury_final_df.rename(columns= {i: \"Player_Address\" for i in  injury_final_df.columns if i.startswith(\"/boxscores\")}, \n",
    "                           inplace=True)\n",
    "    injury_final_df.to_csv(f'../data/raw-data/nfl-injury-report-{s}-raw.csv',\n",
    "                           index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Injury Reports {s}',\n",
    "          f'Time to complete: {end-start}',\n",
    "          sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ddab27-6e66-430a-9681-f91983ef2ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scrape player weekly stats\n",
    "def player_stats(s):\n",
    "    start = time.time()\n",
    "    players = pd.read_csv('../data/database-players.csv')\n",
    "    players = players[players['Last_Year'] >= s]\n",
    "    player_stats_df = pd.DataFrame()\n",
    "    player_fan_df = pd.DataFrame()\n",
    "    for p_add in players['Address']:\n",
    "        try:\n",
    "            df = pd.read_html(f'https://www.pro-football-reference.com{p_add}/gamelog/{s}/', \n",
    "                              header=[0, 1], \n",
    "                              attrs={'id': 'stats'})\n",
    "            df = df[0]\n",
    "            df = df.head(-1)\n",
    "            df.columns = df.columns.to_flat_index()\n",
    "            df['Player_Address'] = p_add\n",
    "            df['Season'] = s\n",
    "            player_stats_df = pd.concat([df, \n",
    "                                         player_stats_df])\n",
    "\n",
    "            df_fan = pd.read_html(f'https://www.pro-football-reference.com{p_add}/fantasy/{s}/', \n",
    "                      header=[0, 1], \n",
    "                      attrs={'id': 'player_fantasy'})\n",
    "            df_fan = df_fan[0]\n",
    "            df_fan = df_fan.head(-1)\n",
    "            df_fan.columns = df_fan.columns.to_flat_index()\n",
    "            df_fan['Player_Address'] = p_add\n",
    "            df_fan['Season'] = s\n",
    "            player_fan_df = pd.concat([df_fan, \n",
    "                                         player_fan_df])\n",
    "        except:\n",
    "            continue\n",
    "    player_stats_df.to_csv(f'../data/raw-data/player-weekly-stats-{s}-raw.csv',\n",
    "                           index=False)\n",
    "    player_fan_df.to_csv(f'../data/raw-data/player-weekly-fantasy-{s}-raw.csv',\n",
    "                           index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Player stats scrape (including snap counts) for {s} season',\n",
    "          f'{(end-start)/60}: minutes to complete.', \n",
    "          sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394eca3a-368d-41f7-9d20-4797a8e5af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_database\n",
    "# for s in range(2015, 2021):\n",
    "#     team_data(s)\n",
    "#     injury_reports(s)\n",
    "#     player_stats(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73817e7-c9c7-471c-a510-a09c7cf02067",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (<ipython-input-2-8dc4d44ace9a>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-8dc4d44ace9a>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "s = 2015\n",
    "players = pd.read_csv('../data/database-players.csv')\n",
    "players = players[players['Last_Year'] >= s]\n",
    "player_fan_df = pd.DataFrame()\n",
    "for p_add in players['Address']:\n",
    "    try:\n",
    "        df_fan = pd.read_html(f'https://www.pro-football-reference.com{p_add}/fantasy/{s}/', \n",
    "                  header=[0, 1], \n",
    "                  attrs={'id': 'player_fantasy'})\n",
    "        df_fan = df_fan[0]\n",
    "        df_fan = df_fan.head(-1)\n",
    "        df_fan.columns = df_fan.columns.to_flat_index()\n",
    "        df_fan['Player_Address'] = p_add\n",
    "        df_fan['Season'] = s\n",
    "        player_fan_df = pd.concat([df_fan, \n",
    "                                     player_fan_df])\n",
    "    except:\n",
    "        continue\n",
    "player_fan_df.to_csv(f'../data/raw-data/player-weekly-fantasy-{s}-raw.csv',\n",
    "                       index=False)\n",
    "end = time.time()\n",
    "print(f'Done: Player stats scrape (including snap counts) for {s} season',\n",
    "      f'{(end-start)/60}: minutes to complete.', \n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b73a8-10fa-48d0-a8a0-4836895aa8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
