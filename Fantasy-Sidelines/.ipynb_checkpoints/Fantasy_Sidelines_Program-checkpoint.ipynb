{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy Sidelines \n",
    "### Code to Scrape, Clean, Save, and Analyze NFL injury, player stats, team stats, and player snaps data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash, requests, os, time, dash_table\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dash.dependencies import Input, Output\n",
    "from dotenv import load_dotenv\n",
    "from dash_table.Format import Format, Scheme, Trim\n",
    "from jupyter_dash import JupyterDash\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Section 1 Scrape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.1 - Team Weekly Stats Scraped From www.stathead.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data_scrape(Season,url):\n",
    "    # scrape weekly team stats from www.stathead.com\n",
    "    start = time.time()\n",
    "    # page to start scrape at\n",
    "    page = 0\n",
    "    # login payload information pulled from a .env file\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    \n",
    "    # lots of team data, could not fit into one pull from stathead, needed to use 2 different url's, this allows the function to iterate through both\n",
    "    if url == 1:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=points&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=E&game_num_min=0&year_min={Season}&cstat[1]=all_td_team&ccomp[1]=gt&cval[1]=0&cstat[2]=third_down_att&ccomp[2]=gt&cval[2]=0&cstat[3]=vegas_line&ccomp[3]=gt&cval[3]=-50&cstat[4]=penalties&ccomp[4]=gt&cval[4]=0&cstat[5]=rush_att&ccomp[5]=gt&cval[5]=0&cstat[6]=tot_yds&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=pass_cmp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url == 2:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=all_td_opp&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={Season}&cstat[1]=tot_yds_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_yds_diff&ccomp[2]=gt&cval[2]=-500&cstat[3]=score_diff_thru_1&ccomp[3]=gt&cval[3]=-500&cstat[4]=rush_att_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=kick_ret_td_tgl&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp_opp&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down_opp&ccomp[7]=gt&cval[7]=0&cstat[8]=score_diff_1_qtr&ccomp[8]=gt&cval[8]=-500&cstat[9]=third_down_att_opp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url != 1 or 2:\n",
    "        print(\"Please select 1 or 2.\")\n",
    "    \n",
    "    # open logged in session for scraping\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        # beginning the scrape and stopping the scrape when page number reaches 100k\n",
    "        try:\n",
    "\n",
    "            while page < 100000:\n",
    "                \n",
    "                # pulling the website and scraping it\n",
    "                website = session.get(stat_url.format(Season=Season,page=page)).text\n",
    "                soup = BeautifulSoup(website, 'html')\n",
    "                table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "                \n",
    "                # pull headers and rows out of the data\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "                \n",
    "                # final location for complete data\n",
    "                final_data = []\n",
    "                \n",
    "                # create row for each line of data in table\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "                \n",
    "                # create the dataframe in panadas excluding the blank row and matching headers with the rows\n",
    "                df = pd.DataFrame(final_data[1:], columns=table_headers[12:])\n",
    "                \n",
    "                # writting dataframe to csv, continuous appending just incase the function fails, data will be saved\n",
    "                if url == 1:\n",
    "                    df.to_csv(f'../data/raw-data/nfl-team-data-{Season}-1-raw.csv',mode='a',index=False)\n",
    "                else:\n",
    "                    df.to_csv(f'../data/raw-data/nfl-team-data-{Season}-2-raw.csv',mode='a',index=False)\n",
    "                \n",
    "                # progress through the websites\n",
    "                page += 100\n",
    "            \n",
    "        except:\n",
    "            # notifying the scrape has completed.\n",
    "            end = time.time()\n",
    "            print(f'Done: Team {Season}, {url}, {page}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.2 - Player Weekly Snap Data Scraped From www.pro-football-reference.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_snap_scrape(Season_Start, Season_End, p_database):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    p_database = p_database.lower()\n",
    "\n",
    "    if p_database == \"y\":\n",
    "\n",
    "        abcd = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "        player_url = \"https://www.pro-football-reference.com/players/{abcd}/\"\n",
    "        players = []\n",
    "        for c in range(0,len(abcd)):\n",
    "            res = requests.get(player_url.format(abcd=abcd[c]))\n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            data = soup.find_all('p')\n",
    "            try:\n",
    "                for i in data:\n",
    "                    a = i.find('a')['href']\n",
    "                    pl = i.get_text()[:(i.get_text().find(\"(\")-1)]\n",
    "                    po = i.get_text()[(i.get_text().find(\"(\")+1):i.get_text().find(\")\")]\n",
    "                    fy = i.get_text()[(i.get_text().rfind(\"-\")-4):(i.get_text().rfind(\"-\"))]\n",
    "                    ly = i.get_text()[(i.get_text().rfind(\"-\")+1):(i.get_text().rfind(\"-\")+5)]\n",
    "                    p = {\"Address\": a[:-4], \n",
    "                         \"Player\": pl, \n",
    "                         \"Position\": po, \n",
    "                         \"First_Year\":  fy, \n",
    "                         \"Last_Year\": ly}\n",
    "                    players.append(p)\n",
    "            except:\n",
    "                pass\n",
    "        players_df = pd.DataFrame().from_dict(players)\n",
    "        players_df = players_df[players_df.Last_Year != \"Ever\"]\n",
    "        players_df['Player'] = players_df['Player'].str.replace('+','')\n",
    "        players_df['Last_Year'] = players_df.Last_Year.astype('int32')\n",
    "        players_df = players_df[players_df.Last_Year >= Season_Start]\n",
    "        players_df.to_csv('../data/database-players.csv',index=False)\n",
    "\n",
    "    players_df = pd.read_csv('../data/database-players.csv')\n",
    "\n",
    "    players_address_list = players_df.Address.tolist()\n",
    "\n",
    "    snap_counts_df = pd.DataFrame()\n",
    "\n",
    "    nfl_weeks = pd.read_csv('../data/NFL-Week-Dates.csv')\n",
    "\n",
    "    for player_address in players_address_list:\n",
    "\n",
    "            try:\n",
    "\n",
    "                df = pd.DataFrame()\n",
    "                df['Player_Address'] = ''\n",
    "                df['Date'] = ''\n",
    "\n",
    "                for s in range(Season_Start, Season_End+1):\n",
    "\n",
    "                    try:\n",
    "                        snap_counts_url = \"https://www.pro-football-reference.com{player_address}/fantasy/{s}\"\n",
    "\n",
    "                        # opening the webpage and storing data into lists then to dataframe\n",
    "                        res = requests.get(snap_counts_url.format(player_address=player_address,s=s))\n",
    "\n",
    "                        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "                        table = soup.find('table', {'id': 'player_fantasy'})\n",
    "                        table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                        table_rows = table.find_all('tr')\n",
    "\n",
    "                        final_data = []\n",
    "                        snap_counts_df = pd.DataFrame()\n",
    "\n",
    "                        for tr in table_rows:\n",
    "                            td = tr.find_all('td')\n",
    "                            row = [tr.text for tr in td]\n",
    "                            final_data.append(row)     \n",
    "\n",
    "                        snap_counts_df = pd.DataFrame(final_data[:-1], columns=table_headers[11:])\n",
    "                        snap_counts_df.columns = ['G#', 'Date', 'Team', 'Home_Away', 'Opp', 'Result', \n",
    "                                                  'Pos', 'Rec_Tgt_In20', 'Rec_Rec_In20', 'Rec_Yds_In20', \n",
    "                                                  'Rec_TD_In20', 'Off_Snaps', 'Off_Snaps%', 'Def_Snaps', \n",
    "                                                  'Def_Snaps%', 'ST_Snaps', 'ST_Snaps%', 'Fntsy_FanPts', \n",
    "                                                  'Fntsy_DKPt', 'Fntsy_FDPt']\n",
    "                        snap_counts_df['Player_Address'] = player_address\n",
    "                        snap_counts_df = snap_counts_df[snap_counts_df.Date >= \"2016-08-01\"]\n",
    "                        snap_counts_df['Date'] = pd.to_datetime(snap_counts_df['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "                        snap_counts_df['Season'] = s\n",
    "                        nfl_weeks['Week'] = nfl_weeks['Week'].astype(str)\n",
    "                        # create function to calculate what nfl week the date occured\n",
    "                        def pre_thu(d):\n",
    "                            days_behind = 3 - d.weekday()\n",
    "                            if days_behind > 0:\n",
    "                                days_behind -= 7\n",
    "                            return d + dt.timedelta(days_behind)\n",
    "                        snap_counts_df['week_start_nfl'] = snap_counts_df['Date'].apply(pre_thu)\n",
    "                        nfl_weeks['Start Date'] = pd.to_datetime(nfl_weeks['Start Date'])\n",
    "                        # merge injury data with the nfl week data to pull the weeks into current datagrame\n",
    "                        snap_counts_df = pd.merge(left=snap_counts_df,right=nfl_weeks,how='left',left_on='week_start_nfl',right_on='Start Date')\n",
    "                        snap_counts_df = snap_counts_df[['Player_Address', 'Pos', 'Team', \n",
    "                                                         'Home_Away', 'Opp', 'Result', \n",
    "                                                         'Date', 'Season', 'Week', 'G#', \n",
    "                                                         'Off_Snaps', 'Off_Snaps%', \n",
    "                                                         'Def_Snaps', 'Def_Snaps%', \n",
    "                                                         'ST_Snaps', 'ST_Snaps%']]\n",
    "                        snap_counts_df.replace({'Home_Away': {'@': 'Away', '': 'Home'}},regex=True,inplace=True)\n",
    "                        snap_counts_df['Off_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['Off_Snaps%'].values))\n",
    "                        snap_counts_df['Def_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['Def_Snaps%'].values))\n",
    "                        snap_counts_df['ST_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['ST_Snaps%'].values))\n",
    "                        snap_counts_df.to_csv(\"../data/raw-data/snap-counts-raw.csv\", mode=\"a\", index=False)\n",
    "\n",
    "                    except:\n",
    "                        snap_counts_url = \"https://www.pro-football-reference.com{player_address}/fantasy/{s}\"\n",
    "\n",
    "                        # opening the webpage and storing data into lists then to dataframe\n",
    "                        res = requests.get(snap_counts_url.format(player_address=player_address,s=s))\n",
    "\n",
    "                        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "                        table = soup.find('table', {'id': 'player_fantasy'})\n",
    "                        table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                        table_rows = table.find_all('tr')\n",
    "\n",
    "                        final_data = []\n",
    "                        snap_counts_df = pd.DataFrame()\n",
    "\n",
    "                        for tr in table_rows:\n",
    "                            td = tr.find_all('td')\n",
    "                            row = [tr.text for tr in td]\n",
    "                            final_data.append(row)\n",
    "\n",
    "                        snap_counts_df = pd.DataFrame(final_data[:-1], columns=table_headers[9:])\n",
    "                        snap_counts_df.columns = ['G#', 'Date', 'Team', 'Home_Away', 'Opp', 'Result', \n",
    "                                                  'Pos', 'Off_Snaps', 'Off_Snaps%', 'Def_Snaps', \n",
    "                                                  'Def_Snaps%', 'ST_Snaps', 'ST_Snaps%', 'Fntsy_FanPts', \n",
    "                                                  'Fntsy_DKPt', 'Fntsy_FDPt']\n",
    "                        snap_counts_df['Player_Address'] = player_address\n",
    "                        snap_counts_df = snap_counts_df[snap_counts_df.Date >= \"2016-08-01\"]\n",
    "                        snap_counts_df['Date'] = pd.to_datetime(snap_counts_df['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "                        snap_counts_df['Season'] = s\n",
    "                        nfl_weeks['Week'] = nfl_weeks['Week'].astype(str)\n",
    "                        # create function to calculate what nfl week the date occured\n",
    "                        def pre_thu(d):\n",
    "                            days_behind = 3 - d.weekday()\n",
    "                            if days_behind > 0:\n",
    "                                days_behind -= 7\n",
    "                            return d + dt.timedelta(days_behind)\n",
    "                        snap_counts_df['week_start_nfl'] = snap_counts_df['Date'].apply(pre_thu)\n",
    "                        nfl_weeks['Start Date'] = pd.to_datetime(nfl_weeks['Start Date'])\n",
    "                        # merge injury data with the nfl week data to pull the weeks into current datagrame\n",
    "                        snap_counts_df = pd.merge(left=snap_counts_df,right=nfl_weeks,how='left',left_on='week_start_nfl',right_on='Start Date')\n",
    "                        snap_counts_df = snap_counts_df[['Player_Address', 'Pos', 'Team', \n",
    "                                                         'Home_Away', 'Opp', 'Result', \n",
    "                                                         'Date', 'Season', 'Week', 'G#', \n",
    "                                                         'Off_Snaps', 'Off_Snaps%', \n",
    "                                                         'Def_Snaps', 'Def_Snaps%', \n",
    "                                                         'ST_Snaps', 'ST_Snaps%']]\n",
    "                        snap_counts_df.replace({'Home_Away': {'@': 'Away', '': 'Home'}},regex=True,inplace=True)\n",
    "                        snap_counts_df['Off_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['Off_Snaps%'].values))\n",
    "                        snap_counts_df['Def_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['Def_Snaps%'].values))\n",
    "                        snap_counts_df['ST_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['ST_Snaps%'].values))\n",
    "                        snap_counts_df.to_csv(\"../data/raw-data/snap-counts-raw.csv\", mode=\"a\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    end = time.time()\n",
    "    print(f'Done: Player Snaps from {Season_Start} to {Season_End}', f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.3 - NFL Weekly Injury Reports Scraped From www.pro-football-reference.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injury_reports_scrape(Season):\n",
    "\n",
    "    # scrape weekly nfl injury reports\n",
    "    start = time.time()\n",
    "    # list of team abbreviations from pro-football-reference for url purposes\n",
    "    teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "             'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "\n",
    "    ENDPOINT = 'https://www.pro-football-reference.com/teams/{team}/{Season}_injuries.htm'\n",
    "    \n",
    "    # creating final place to store data\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    # scrape pages for all teams\n",
    "    for team in teams:\n",
    "        \n",
    "        # open webpage and scrape contents into lists, then to a dataframe\n",
    "        res = requests.get(ENDPOINT.format(Season=Season,team=team))\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "        table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "        table_rows = table.find_all('tr')\n",
    "\n",
    "        final_data = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all(['th','td'])\n",
    "            row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "            final_data.append(row)\n",
    "\n",
    "        dfdata = final_data[1:]\n",
    "        data_body = [[dfdata[j][i] for j in range(len(dfdata))] for i in range(len(dfdata[0]))]\n",
    "\n",
    "        df = pd.DataFrame(data_body,final_data[0]).T\n",
    "        # adding team and season columns for identification\n",
    "        df.insert(loc=1,column='Team',value=team)\n",
    "        df.insert(loc=2,column='Season',value=Season)\n",
    "\n",
    "        # combine current data with final dataframe\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # rename column\n",
    "    final_df.rename(columns={'PlayerÂ ':'Player'},inplace=True)\n",
    "    \n",
    "    # write final data to csv file\n",
    "    final_df.to_csv(f'../data/raw-data/nfl-injury-report-{Season}-raw.csv',index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Injury Reports {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.4 - Player Weekly Stats Scraped From www.stathead.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_stats_scape(Season):\n",
    "    \n",
    "    # scrape weekly player stats from statehead.com\n",
    "    start = time.time()\n",
    "    # page/location for monitoring progress and continuing through pages\n",
    "    page = 0\n",
    "    location = 2000\n",
    "    \n",
    "    # stathead login info pulled from .env file\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    stat_url = \"https://stathead.com/football/pgl_finder.cgi?request=1&game_num_max=99&week_num_max=99&order_by=all_td&season_start=1&qb_gwd=0&order_by_asc=0&qb_comeback=0&week_num_min=0&game_num_min=0&year_min={Season}&match=game&year_max={Season}&season_end=-1&age_min=0&game_type=R&age_max=99&positions[]=qb&positions[]=rb&positions[]=wr&positions[]=te&positions[]=e&positions[]=t&positions[]=g&positions[]=c&positions[]=ol&positions[]=dt&positions[]=de&positions[]=dl&positions[]=ilb&positions[]=olb&positions[]=lb&positions[]=cb&positions[]=s&positions[]=db&positions[]=k&positions[]=p&cstat[1]=punt_ret&ccomp[1]=gt&cval[1]=0&cstat[2]=sacks&ccomp[2]=gt&cval[2]=0&cstat[3]=fumbles&ccomp[3]=gt&cval[3]=0&cstat[4]=rush_att&ccomp[4]=gt&cval[4]=0&cstat[5]=pass_defended&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp&ccomp[6]=gt&cval[6]=0&cstat[7]=targets&ccomp[7]=gt&cval[7]=0&cstat[8]=kick_ret&ccomp[8]=gt&cval[8]=0&offset={page}\"\n",
    "\n",
    "    # logging into session to begin scrape\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        # begin scrape, once it fails, stop scrape\n",
    "        try:\n",
    "\n",
    "            # scrape webpages up to 100k contents\n",
    "            while page < 100000:\n",
    "\n",
    "                # opening webpage and storing data into list then dataframe\n",
    "                website = session.get(stat_url.format(Season=Season,page=page)).text\n",
    "                soup = BeautifulSoup(website, 'html')\n",
    "                table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "\n",
    "                final_data = []\n",
    "\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "\n",
    "                df = pd.DataFrame(final_data[1:], columns=table_headers[11:])\n",
    "                df.rename(columns={'Year':'Season'},inplace=True)\n",
    "                \n",
    "                # appendings csv with data from current dataframe to prevent loss if code fails or connection drops\n",
    "                df.to_csv(f'../data/raw-data/player-stats-{Season}-raw.csv',mode='a',index=False)\n",
    "\n",
    "                # continue through webpages and update where the location of the scrape is\n",
    "                if page > location:\n",
    "                    print('Player Stats on page:',(page-100))\n",
    "                    location += 2000\n",
    "\n",
    "                page += 100\n",
    "                \n",
    "        except:\n",
    "            # end of the scrape notificaiton\n",
    "            end = time.time()\n",
    "            print(f'Done: Stats {Season}, {page}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Section 2 - Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.1 - Clean Team Weekly Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data_clean(Season):\n",
    "    # begin cleaning team data\n",
    "    start = time.time()\n",
    "    # opening both csv files into dataframes for cleaning and combining\n",
    "    team1 = pd.read_csv(f'../data/raw-data/nfl-team-data-{Season}-1-raw.csv')\n",
    "    team2 = pd.read_csv(f'../data/raw-data/nfl-team-data-{Season}-2-raw.csv')\n",
    "    # drop all blank rows\n",
    "    team1.dropna(thresh=10,inplace=True)\n",
    "    team2.dropna(thresh=10,inplace=True)\n",
    "    # drop useless columns\n",
    "    team1.drop('LTime',axis=1,inplace=True)\n",
    "    team2.drop(['LTime'],axis=1,inplace=True)\n",
    "    # rename columns\n",
    "    team1.rename(columns={'Tm':'Team','Unnamed: 5':'Away_Home','PF':'Points_For','PA':'Points_Against','PC':'Points_Comb',\\\n",
    "                         'vs. Line':'Vs_Line','Cmp':'TPass_Cmp','Att':'TPass_Att','Cmp%':'TPass_Cmp%','Yds':'TPass_Yds',\\\n",
    "                          'TD':'TPass_TD','Int':'TPass_Int','Sk':'TSack','Yds.1':'TSack_Yds','Rate':'TQB_Rate',\\\n",
    "                          'Att.1':'TRush_Att','Yds.2':'TRush_Yds','Y/A':'TRush_Y/A','TD.1':'TRush_TD','Tot':'TTot_Yds',\\\n",
    "                          'Ply':'TO_Play#','Y/P':'TO_Y/P','DPly':'TD_Play#','DY/P':'TD_Y/P','TO':'TTot_TO','ToP':'TO_ToP',\\\n",
    "                          'Time.1':'TGame_Dur','Yds.3':'TPen_Yds','OppPen':'TOpp_Pen','OppYds':'TOpp_Pen_Yds',\\\n",
    "                          'CombPen':'TComb_Pen','CombPenYds':'TComb_Pen_Yds','1stD':'T1st_Downs','Rsh':'T1st_by_Rsh',\\\n",
    "                          'Pass':'T1st_by_Pass','Pen.1':'T1st_by_Pen','3DAtt':'T3rd_Down_Att','3DConv':'T3rd_Down_Conv',\\\n",
    "                          '3D%':'T3rd_Down%','4DAtt':'T4th_Down_Att','4DConv':'T4th_Down_Conv','4D%':'T4th_Down%',\\\n",
    "                          'TD.2':'TTot_TD','XPA':'TXP_Att','XPM':'TXP_Made','FGA':'TFG_Att','FGM':'TFG_Made','2PA':'T2Pt_Att',\\\n",
    "                          '2PM':'T2Pt_Made','Sfty':'TSfty','Pnt':'TTimes_Punted','Yds.4':'TPunt_Yds','Y/P.1':'TPunt_Yds_Avg','Year':'Season'},inplace=True)\n",
    "    team2.rename(columns={'Tm':'Team','Unnamed: 5':'Away_Home','TD':'TOpp_Tot_TD','XPA':'TOpp_XP_Att','XPM':'TOpp_XP_Made',\\\n",
    "                          'Att':'TOpp_FG_Att','Md':'TOpp_FG_Made','Sfty':'TOpp_Sfty','Cmp':'TOpp_Pass_Cmp','Att.1':'TOpp_Pass_Att',\\\n",
    "                          'Cmp%':'TOpp_Pass_Cmp%','Yds':'TOpp_Pass_Yds','TD.1':'TOpp_Pass_TD','Int':'TOpp_Pass_Int','Sk':'TOpp_Sk',\\\n",
    "                          'Yds.1':'TOpp_Sk_Yds','Rate':'TOpp_QB_Rate','Att.2':'TOpp_Rush_Att','Yds.2':'TOpp_Rush_Yds',\\\n",
    "                          'Y/A':'TOpp_Rush_Y/A','TD.2':'TOpp_Rush_TD','Tot':'TOpp_Tot_Yds','TO':'TOpp_Tot_TO',\\\n",
    "                          '1stDOpp':'TOpp_1st_Downs','Rush':'TOpp_1st_by_Rsh','Pass':'TOpp_1st_by_Pass','Pen':'TOpp_1st_by_Pen',\\\n",
    "                          'Opp3DAtt':'TOpp_3rd_Down_Att','Opp3DConv':'TOpp_3rd_Down_Conv','Opp3D%':'TOpp_3rd_Down%',\\\n",
    "                          'Opp4DAtt':'TOpp_4th_Down_Att','Opp4DConv':'TOpp_4th_Down_Conv','Opp4D%':'TOpp_4th_Down%',\\\n",
    "                          'Rush.1':'TMargin_Rush','Pass.1':'TMargin_Pass','Tot.1':'TMargin_TotYds','TO.1':'TTO_TD',\\\n",
    "                          'KR':'TKR_TD','PR':'TPR_TD','IR':'TInt_TD','FR':'TFmb_TD','OR':'TOtherRet_TD',\\\n",
    "                          'RetTD':'TAll_Ret_TD','Q1':'TMar_Thru_Q1','Q2':'TMar_Thru_Q2','Q3':'TMar_Thru_Q3',\\\n",
    "                          'Q1.1':'TScore_Diff_Q1','Q2.1':'TScore_Diff_Q2','Q3.1':'TScore_Diff_Q3',\\\n",
    "                          'Q4':'TScore_Diff_Q4','1stHalf':'TScore_Diff_1stHalf','2ndHalf':'TScore_Diff_2ndHalf','Year':'Season'},inplace=True)\n",
    "    \n",
    "    # merge the two dataframes based on team, season, date, time, away/home, opp, week, g#, dat, result, and ot\n",
    "    team = pd.merge(left=team1,right=team2,\\\n",
    "                     how='outer',\\\n",
    "                     on=['Team','Season','Date','Time','Away_Home','Opp','Week','G#','Day','Result','OT'])\n",
    "    team.set_index('Team',inplace=True)\n",
    "    # drop all rows that have headers in them\n",
    "    team.drop('Tm',inplace=True)\n",
    "    team.reset_index(inplace=True)\n",
    "\n",
    "    # create a list of all the column names in the team dataframe\n",
    "    team_cols = []\n",
    "    \n",
    "    for col in team.columns:\n",
    "        team_cols.append(col)\n",
    "\n",
    "    # cleaning the away/home column to show away or home instead of @\n",
    "    team.replace({'Away_Home':{'@':'Away',None:'Home'}},inplace=True)\n",
    "    team[team_cols[11:]] = team[team_cols[11:]].fillna(value=0)\n",
    "    # change datatype of the columns\n",
    "    team[['TPass_Cmp','TPass_Att','T3rd_Down_Att','T3rd_Down_Conv','T4th_Down_Att',\\\n",
    "          'T4th_Down_Conv','TOpp_Pass_Cmp','TOpp_Pass_Att','TOpp_3rd_Down_Att',\\\n",
    "          'TOpp_3rd_Down_Conv','TOpp_4th_Down_Att','TOpp_4th_Down_Conv']] = team[['TPass_Cmp','TPass_Att','T3rd_Down_Att',\\\n",
    "                                                                                  'T3rd_Down_Conv','T4th_Down_Att',\\\n",
    "                                                                                  'T4th_Down_Conv','TOpp_Pass_Cmp',\\\n",
    "                                                                                  'TOpp_Pass_Att','TOpp_3rd_Down_Att',\\\n",
    "                                                                                  'TOpp_3rd_Down_Conv','TOpp_4th_Down_Att',\\\n",
    "                                                                                  'TOpp_4th_Down_Conv']].astype(float)\n",
    "    # calculating the columns to show apporpriate values\n",
    "    team['TPass_Cmp%'] = team['TPass_Cmp']/team['TPass_Att']\n",
    "    team['T3rd_Down%'] = team['T3rd_Down_Conv']/team['T3rd_Down_Att']\n",
    "    team['T4th_Down%'] = team['T4th_Down_Conv']/team['T4th_Down_Att']\n",
    "    team['TOpp_Pass_Cmp%'] = team['TOpp_Pass_Cmp']/team['TOpp_Pass_Att']\n",
    "    team['TOpp_3rd_Down%'] = team['TOpp_3rd_Down_Conv']/team['TOpp_3rd_Down_Att']\n",
    "    team['TOpp_4th_Down%'] = team['TOpp_4th_Down_Conv']/team['TOpp_4th_Down_Att']\n",
    "    # changing dates/time to datetime values\n",
    "    team['Date'] = pd.to_datetime(team['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "    team['TGame_Dur'] = team['TGame_Dur']+':00'\n",
    "    team['TO_ToP'] = '00:'+team['TO_ToP']\n",
    "    team['TGame_Dur'] = pd.to_timedelta(team['TGame_Dur'],errors='coerce')\n",
    "    team['TGame_Dur'] = team['TGame_Dur'].dt.total_seconds()\n",
    "    team['TO_ToP'] = pd.to_timedelta(team['TO_ToP'],errors='coerce')\n",
    "    team['TO_ToP'] = team['TO_ToP'].dt.total_seconds()\n",
    "    #changing datatypes of columns\n",
    "    team[team_cols[11:16]] = team[team_cols[11:16]].astype(float)\n",
    "    team[team_cols[17]] = team[team_cols[17]].astype(float)\n",
    "    team[team_cols[19:]] = team[team_cols[19:]].astype(float)\n",
    "    # adding month column\n",
    "    team.insert(loc=9,column='Month',value=team['Date'].dt.month)\n",
    "    # chaning week datatype and removing any week larger than week 17\n",
    "    team['Week'] = team['Week'].astype(int)\n",
    "    team = team[team['Week']<=17]\n",
    "    team['Week'] = team['Week'].astype(str)\n",
    "    # saving cleaned data to csv\n",
    "    team.to_csv(f'../data/clean-data/nfl-team-data-{Season}-clean.csv',index=False)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Team data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.2 - Clean Player Weekly Snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def player_snaps_clean(Season):\n",
    "    \n",
    "#     start = time.time()\n",
    "#     snaps = pd.read_csv(f'../data/raw-data/snap-counts{Season}-raw.csv')\n",
    "#     # drop unnecessary columns\n",
    "#     snaps.drop(['Fantasy Pts','Pts/100 Snaps','Rush %','Tgt %','Touch %','Util %','Tackle %','Sack %','QB Hit %','Snaps/Gm'],axis=1,inplace=True)\n",
    "#     # change datatypes and calculate columns approriately\n",
    "#     snaps['Season'] = snaps['Season'].astype(str)\n",
    "#     snaps['Snap %'] = snaps['Snap %']/100\n",
    "#     snaps['Week'] = snaps['Week'].astype(str)\n",
    "#     # change team names to be consistent\n",
    "#     snaps.replace({'Team':{'FA':'','GB':'GNB','JAC':'JAX','KC':'KAN','NE':'NWE','NO':'NOR','SF':'SFO','TB':'TAM'}},inplace=True)\n",
    "#     # cleaning player names to be consistent\n",
    "#     snaps['Player'] = snaps['Player']+' '\n",
    "#     snaps.replace({'Player':{' Jr. ':'',' Jr ':'',' Sr. ':'',' Sr ':'',' III ':'',' II ':'',' IV ':'',' V ':''}},regex=True,inplace=True)\n",
    "#     snaps['Player'] = snaps['Player'].str.strip(' ')\n",
    "#     snaps['Player'] = snaps['Player'].str.replace('.','')\n",
    "#     # save cleaned snap data\n",
    "#     snaps.to_csv(f'../data/clean-data/snap-counts-{Season}-clean.csv',index=False)\n",
    "#     end = time.time()\n",
    "\n",
    "#     print(f'Snap data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.3 - Clean NFL Weekly Injury Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfl_injury_clean(Season):\n",
    "    \n",
    "    start = time.time()\n",
    "    injury = pd.read_csv(f'../data/raw-data/nfl-injury-report-{Season}-raw.csv',low_memory=False)\n",
    "    # separate file to help calculate week numbers\n",
    "    nfl_weeks = pd.read_csv('../data/NFL-Week-Dates.csv')\n",
    "    # melt columns to break each week into a separate row\n",
    "    injury = pd.melt(injury,id_vars=['Player','Team','Season'],var_name='Date', value_name='Status')\n",
    "    # create 2 columns based off a header\n",
    "    injury[['Date','Opp']] = injury.Date.str.split('vs. ',expand=True)\n",
    "    injury[['Month','Day']] = injury.Date.str.split('/',expand=True)\n",
    "    injury[['Status','Injury']] = injury.Status.str.split(\":\",expand=True)\n",
    "    # drop all values that are empty after the melting/splitting\n",
    "    injury.dropna(axis=0,subset=['Status','Injury'],how='all',inplace=True)\n",
    "    # change datatypes for cleaning\n",
    "    injury[['Season','Month','Day']] = injury[['Season','Month','Day']].astype(int)\n",
    "    # combine datat to create a date for the game and change to datetime value\n",
    "    injury['Date'] = injury['Date']+'/'+(np.where(injury['Month']<=2,injury['Season']+1,injury['Season'])).astype(str)\n",
    "    injury['Date'] = pd.to_datetime(injury['Date'])\n",
    "    nfl_weeks['Week'] = nfl_weeks['Week'].astype(str)\n",
    "    # create function to calculate what nfl week the date occured\n",
    "    def pre_thu(d):\n",
    "        days_behind = 3 - d.weekday()\n",
    "        if days_behind > 0:\n",
    "            days_behind -= 7\n",
    "        return d + dt.timedelta(days_behind)\n",
    "    injury['week_start_nfl'] = injury['Date'].apply(pre_thu)\n",
    "    nfl_weeks['Start Date'] = pd.to_datetime(nfl_weeks['Start Date'])\n",
    "    # merge injury data with the nfl week data to pull the weeks into current datagrame\n",
    "    injury = pd.merge(left=injury,right=nfl_weeks,how='left',left_on='week_start_nfl',right_on='Start Date')\n",
    "    # update team names for consistency\n",
    "    injury.replace({'Team':\\\n",
    "                       {'crd':'ARI', 'atl':'ATL', 'rav':'BAL', 'buf':'BUF', 'car':'CAR', 'chi':'CHI', 'cin':'CIN',\\\n",
    "                        'cle':'CLE', 'dal':'DAL', 'den':'DEN', 'det':'DET', 'gnb':'GNB','htx':'HOU','clt':'IND',\\\n",
    "                        'jax':'JAX','kan':'KAN','sdg':'LAC','ram':'LAR','mia':'MIA','min':'MIN','nor':'NOR','nwe':'NWE',\\\n",
    "                        'nyg':'NYG','nyj':'NYJ','rai':'OAK','phi':'PHI','pit':'PIT','sea':'SEA','sfo':'SFO','tam':'TAM',\\\n",
    "                        'oti':'TEN','was':'WAS'}},inplace=True)\n",
    "    # clean injury data for easier management and searching of data\n",
    "    injury['Injury'] = injury['Injury'].str.strip(' ')\n",
    "    injury.replace({'Injury':{'right':'','left':'','Right':'','Left':'','Biceps':'Bicep',\\\n",
    "                              'Triceps':'Tricep','Ankles':'Ankle','hip':'Hip','Hips':'Hip','Knees':'Knee',\\\n",
    "                              'Virus':'Illness','Triceps':'Tricep','Oblique':'Abdomen',\\\n",
    "                              'NotInjuryRelated':'Not Injury Related','MedicalIllness':'Illness',\\\n",
    "                              'LowerLeg':'Lower Leg','CoreMuscle':'Abdomen','Abdominal':'Abdomen'}},\\\n",
    "                               regex=True,inplace=True)\n",
    "    # clean player names for consistency\n",
    "    injury['Player'] = injury['Player']+' '\n",
    "    injury.replace({'Player':{' Jr. ':'',' Jr ':'',' Sr. ':'',' Sr ':'',' III ':'',' II ':'',' IV ':'',' V ':''}},regex=True,inplace=True)\n",
    "    injury['Player'] = injury['Player'].str.strip(' ')\n",
    "    injury['Player'] = injury['Player'].str.replace('.','')\n",
    "    # drop unnecessary columns\n",
    "    injury.drop(['Month','Day','week_start_nfl','Start Date'],axis=1,inplace=True)\n",
    "    # drop all empty week values\n",
    "    injury.dropna(subset=['Week'],inplace=True)\n",
    "    # change datatypes\n",
    "    injury[['Player','Week','Season']] = injury[['Player','Week','Season']].astype(str)\n",
    "    # reorganize columns\n",
    "    injury = injury[['Player','Team','Opp','Date','Season','Week','Status','Injury']]\n",
    "    # add two empty columns for data input\n",
    "    injury[['Specific_Inj','Side']] = None\n",
    "    injury.to_csv(f'../data/clean-data/injury-report-{Season}-clean.csv',index=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Injury data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.4 - Clean Player Weekly Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_stats_clean(Season):\n",
    "    \n",
    "    start = time.time()\n",
    "    stats = pd.read_csv(f'../data/raw-data/player-stats-{Season}-raw.csv')\n",
    "    # drop empty rows\n",
    "    stats.dropna(how='all',inplace=True)\n",
    "    # drop all rows with header information\n",
    "    stats.drop(stats[stats['Player'] == 'Player'].index, inplace = True)\n",
    "    stats.drop('Lg',axis=1,inplace=True)\n",
    "    # rename columns for better management\n",
    "    stats.rename(columns={'Tm':'Team','Unnamed: 6':'Away_Home','Cmp':'IPass_Cmp','Att':'IPass_Att','Cmp%':'IPass_Cmp%','Yds':'IPass_Yds',\\\n",
    "                 'TD':'IPass_TD','Int':'IPass_Int','Rate':'IQB_Rate','Sk':'I_Sk','Yds.1':'ISk_Yds','Y/A':'IPass_Y/A',\\\n",
    "                 'AY/A':'IPass_AdjY/A','Att.1':'IRush_Att','Yds.2':'IRush_Yds','Y/A.1':'IRush_Y/A','TD.1':'IRush_TD',\\\n",
    "                 'Tgt':'IRec_Tgt','Rec':'IRec_Rec','Yds.3':'IRec_Yds','Y/R':'IRec_Y/R','TD.2':'IRec_TD','Ctch%':'IRec_Ctch%',\\\n",
    "                 'Y/Tgt':'IRec_Y/Tgt','XPM':'IXP_Made','XPA':'IXP_Att','XP%':'IXP%','FGM':'IFG_Made','FGA':'IFG_Att',\\\n",
    "                 'FG%':'IFG%','2PM':'I2pt_Made','Sfty':'ISfty','TD.3':'ITot_TD','Pts':'ITot_Pts','Rt':'IKR_Rt','Yds.4':'IKR_Yds',\\\n",
    "                 'Y/Rt':'IKR_Y/Rt','TD.4':'IKR_TD','Ret':'IPR_Rt','Yds.5':'IPR_Yds','Y/R.1':'IPR_Y/Rt','TD.5':'IPR_TD',\\\n",
    "                 'Sk.1':'ITack_Sk','Solo':'ITack_Solo','Ast':'ITack_Ast','Comb':'ITack_Tot','TFL':'ITack_TFL',\\\n",
    "                 'QBHits':'ITack_QBHits','Int.1':'IDef_Int','Yds.6':'IDef_IntYds','TD.6':'IDef_IntTD','PD':'IDef_PD',\\\n",
    "                 'Fmb':'IFmb_Fmb','FL':'IFmb_Lost','FF':'IFmb_Forced','FR':'IFmb_Recov','Yds.7':'IFmb_Yds','TD.7':'IFmb_TD'},\\\n",
    "                 inplace=True)\n",
    "    # create a list of the columns\n",
    "    stats_cols = []\n",
    "    for col in stats.columns:\n",
    "        stats_cols.append(col)\n",
    "    # replace blanks and @ with home and away\n",
    "    stats.replace({'Away_Home':{'@':'Away',None:'Home'}},inplace=True)\n",
    "    # change datatypes and calculate data appropriately\n",
    "    stats[['IPass_Cmp','IPass_Att','IRec_Rec','IRec_Tgt','IXP_Made','IXP_Att','IFG_Made','IFG_Att']] = stats[['IPass_Cmp','IPass_Att','IRec_Rec','IRec_Tgt','IXP_Made','IXP_Att','IFG_Made','IFG_Att']].astype(float)\n",
    "    stats['IPass_Cmp%'] = stats['IPass_Cmp']/stats['IPass_Att']\n",
    "    stats['IRec_Ctch%'] = stats['IRec_Rec']/stats['IRec_Tgt']\n",
    "    stats['IXP%'] = stats['IXP_Made']/stats['IXP_Att']\n",
    "    stats['IFG%'] = stats['IFG_Made']/stats['IFG_Att']\n",
    "    # change datatypes\n",
    "    stats[stats_cols[11:]] = stats[stats_cols[11:]].astype(float)\n",
    "    stats[stats_cols[11:]] = stats[stats_cols[11:]].fillna(value=0)\n",
    "    stats['Date'] = pd.to_datetime(stats['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "    # insert a column for season\n",
    "    stats.insert(loc=4,column='Season',value=Season)\n",
    "    stats['Player'] = stats['Player'].astype(str)\n",
    "    stats['Week'] = stats['Week'].astype(str)\n",
    "    stats['Season'] = stats['Season'].astype(str)\n",
    "    # clean player names for consistency\n",
    "    stats['Player'] = stats['Player']+' '\n",
    "    stats.replace({'Player':{' Jr. ':'',' Jr ':'',' Sr. ':'',' Sr ':'',' III ':'',' II ':'',' IV ':'',' V ':''}},regex=True,inplace=True)\n",
    "    stats['Player'] = stats['Player'].str.strip(' ')\n",
    "    stats['Player'] = stats['Player'].str.replace('.','')\n",
    "    stats.to_csv(f'../data/clean-data/player-stats-{Season}-clean.csv',index=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Player stats data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Section 3 - Grouped Run Fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 3.1 - Scrape All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scrape(Start,End):\n",
    "    # run all web scrape functions at once for multiple season\n",
    "    start = time.time()\n",
    "    season = []\n",
    "    for yr in range(Start,End+1):\n",
    "        season.append(yr)\n",
    "    for s in season:\n",
    "        team_data_scrape(s,1)\n",
    "        team_data_scrape(s,2)\n",
    "        injury_reports_scrape(s)\n",
    "        player_stats_scape(s)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Scrape complete.',f'Time to complete season {s}: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 3.2 - Clean All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clean(Start,End):\n",
    "    # run all data clean functions at once for multiple seasons\n",
    "    start = time.time()\n",
    "    season = []\n",
    "    for yr in range(Start,End+1):\n",
    "        season.append(yr)\n",
    "    for s in season:\n",
    "        team_data_clean(s)\n",
    "        nfl_injury_clean(s)\n",
    "        player_stats_clean(s)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Data clean complete.',f'Time to compelte season {s}: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_scrape(2016,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_clean(2016,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_snap_scrape(2016,2020,\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Section 4 - Prepare for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 4.1 - Combine all files of one season into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "def combine_season(Season):\n",
    "\n",
    "    # open csv files for combining data\n",
    "    inj = pd.read_csv(f'../data/clean-data/injury-report-{Season}-clean.csv')\n",
    "    stat = pd.read_csv(f'../data/clean-data/player-stats-{Season}-clean.csv')\n",
    "    snp = pd.read_csv(f'../data/clean-data/snap-counts-{Season}-clean.csv')\n",
    "    tm = pd.read_csv(f'../data/clean-data/nfl-team-data-{Season}-clean.csv')\n",
    "    \n",
    "    # combine stat and snap data\n",
    "    ss = pd.merge(left=stat,right=snp,how='outer',on=['Player','Season','Week'])\n",
    "    ss['Team_x'] = ss['Team_x'].fillna(ss['Team_y'])\n",
    "    ss['Pos_x'] = ss['Pos_y'].fillna(ss['Pos_x'])\n",
    "    ss.drop(['Team_y','Pos_y'], axis=1, inplace=True)\n",
    "    ss.rename(columns={'Team_x':'Team','Pos_x':'Pos'},inplace=True)\n",
    "    \n",
    "    # combine stat_snap data and injury data\n",
    "    ssj = pd.merge(left=ss,right=inj,how='outer',on=['Player','Season','Week'])\n",
    "    ssj['Date_x'] = ssj['Date_x'].fillna(ssj['Date_y'])\n",
    "    ssj['Team_x'] = ssj['Team_x'].fillna(ssj['Team_y'])\n",
    "    ssj['Opp_x'] = ssj['Opp_x'].fillna(ssj['Opp_y'])\n",
    "    ssj.drop(['Date_y','Team_y','Opp_y','Games'],axis=1,inplace=True)\n",
    "    ssj.rename(columns={'Date_x':'Date','Team_x':'Team','Opp_x':'Opp'},inplace=True)\n",
    "    ssj['Player'] = ssj['Player'].str.replace('.','')\n",
    "    \n",
    "    # combine team data into the individual data to fill in some blanks\n",
    "    team = tm['Team'].unique().tolist()\n",
    "    team = team*17\n",
    "    team.sort()\n",
    "    team_df = pd.DataFrame(team, columns=['Team'])\n",
    "    team_df['Season'] = Season\n",
    "    team_df['Opp'] = \"Bye\"\n",
    "    team_df['Week'] = team_df.groupby('Team').cumcount()+1\n",
    "    tm = pd.merge(left=tm, right=team_df, how=\"outer\", on=['Team', 'Season', 'Week'])\n",
    "    tm['Opp_x'] = tm['Opp_x'].fillna(tm['Opp_y'])\n",
    "    tm.rename(columns={'Opp_x':'Opp'}, inplace=True)\n",
    "    tm.drop('Opp_y', axis=1, inplace=True)\n",
    "    ssjt = pd.merge(left=ssj,right=tm,how='outer',on=['Team','Season','Week'])\n",
    "    ssjt['Date_x'] = ssjt['Date_x'].fillna(ssjt['Date_y'])\n",
    "    ssjt['Away_Home_x'] = ssjt['Away_Home_x'].fillna(ssjt['Away_Home_y'])\n",
    "    ssjt['Opp_x'] = ssjt['Opp_x'].fillna(ssjt['Opp_y'])\n",
    "    ssjt['G#_x'] = ssjt['G#_x'].fillna(ssjt['G#_y'])\n",
    "    ssjt['Day_x'] = ssjt['Day_x'].fillna(ssjt['Day_y'])\n",
    "    ssjt['Result_x'] = ssjt['Result_x'].fillna(ssjt['Result_y'])\n",
    "    ssjt.drop(['Date_y','Away_Home_y','Opp_y','G#_y','Day_y','Result_y'],axis=1,inplace=True)\n",
    "    ssjt.rename(columns={'Date_x':'Date','Away_Home_x':'Away_Home','Opp_x':'Opp','G#_x':'G#','Day_x':'Day','Result_x':'Result'},inplace=True)\n",
    "    \n",
    "    # sort columns for easier viewing\n",
    "    ssjt.sort_values(by=['Player','Season','Week'],inplace=True)\n",
    "    \n",
    "    # give/change all player positions to players with positions listed for the current year\n",
    "    ssjt.replace({'Pos':{'0':None}},inplace=True)\n",
    "    key = ssjt[['Player','Pos']]\n",
    "    key = key.drop_duplicates()\n",
    "    key = key.dropna()\n",
    "    key.replace({'Pos':{'DT':'DL','CB':'DB','FB':'RB','OLB':'LB','S':'DB','G':'OL','DE':'DL','FS':'DB','RT':'OL','T':'OL','RG':'OL','LT':'OL','C':'OL','LS':'LongSnap','LDE':'DL','RILB':'LB','LCB':'DB','ILB':'LB',\\\n",
    "                       'MLB':'LB','TB':'RB','LDT':'DL','NT':'DL','WILL':'LB','SS':'DB','RDE':'DL','SAM':'LB','HB':'RB','WR/RS':'WR','G/T':'OL','C/G':'OL','G/C':'OL','OT':'OL','CB/RS':'DB','DB/LB':'LB','T/G':'OL',\\\n",
    "                        'RB/WR':'RB','LB/FB':'LB','DE/LB':'LB','DT/DE':'DL','TE/DE':'TE','K/P':'K','OS':'OL','LG':'OL','MIKE':'LB','LILB':'LB','WLB':'LB','G/OT':'OL','SLB':'LB','RCB':'DB'}},inplace=True)\n",
    "    ssjt = pd.merge(ssjt,key,how='left',left_on='Player',right_on='Player')\n",
    "    ssjt['Pos_x'] = ssjt['Pos_y']\n",
    "    ssjt.drop(['Pos_y'],axis=1,inplace=True)\n",
    "    ssjt.rename(columns={'Pos_x':'Pos'},inplace=True)\n",
    "    ssjt.dropna(subset=['Player'],inplace=True)\n",
    "    player = ssjt['Player'].unique().tolist()\n",
    "    player = player*17\n",
    "    player.sort()\n",
    "    df2 = pd.DataFrame(player,columns=['Player'])\n",
    "    df2['Season'] = Season\n",
    "    df2['Week'] = df2.groupby('Player').cumcount()+1\n",
    "    ssjt = pd.merge(ssjt,df2,how='outer', on=['Player', 'Season', 'Week'])\n",
    "    ssjt['Injury'] = ssjt['Injury'].str.lower()\n",
    "    ssjt['Injury'] = ssjt['Injury'].str.strip()\n",
    "    ssjt.replace({'Injury':\n",
    "                         {'biceps':'bicep','abdominal':'abdomen','core':'abdomen',\"broken fibula\":'fibula','fractured forearm':'forearm',\\\n",
    "                          'general medical issue':'illness','quadriceps':'quadricep','ribs':'rib','rib cage':'rib','glute':'hip',\\\n",
    "                          'sprained shoulder':'sprained ac joint','thumb sprain':'thumb','nack':'neck','nack, groin':'neck, groin',\\\n",
    "                          'torn acl':'acl','torn mcl':'mcl','oblique':'abdomen'}},inplace=True)\n",
    "\n",
    "    # write data frame to csv\n",
    "    ssjt.to_csv(f'../data/clean-data/nfl-{Season}.csv',index=False)\n",
    "\n",
    "def combine_all_season(Start,End):\n",
    "    start = time.time()\n",
    "    for yr in range(Start,End+1):\n",
    "        combine_season(yr)\n",
    "    end = time.time()\n",
    "    print(f'Data clean combined and cleaned further.',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_all_season(2016, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 4.2 - Split data after the player sustained an injury for the remainder of the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injured_database(Start,End):\n",
    "    # create a database of all athletes that sustained an injury and what their production was post injury for the remainder of the season\n",
    "    start = time.time()\n",
    "    \n",
    "    season = []\n",
    "    for yr in range(Start,End+1):\n",
    "        season.append(yr)\n",
    "        \n",
    "    for s in season:\n",
    "        df = pd.read_csv(f'../data/clean-data/nfl-{s}.csv',low_memory=False)\n",
    "        df.sort_values(['Player','Week'],inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.drop(['index'],axis=1,inplace=True)\n",
    "\n",
    "        # create list of status's to be able to identify were to split the rows\n",
    "        status = df['Status'].dropna().unique().tolist()\n",
    "\n",
    "        # create a database to located the index of the first instance of an injury\n",
    "        Player = df.loc[df['Status'].isin(status)].drop_duplicates(subset=['Player']).index.unique().tolist()\n",
    "\n",
    "        # split out the rows then save them to a separate dataframe and export to csv\n",
    "        inj_df = pd.DataFrame()\n",
    "\n",
    "        for i in range(0,len(Player)):\n",
    "            idx = df[df['Player']==df['Player'][Player[i]]].tail(1)\n",
    "            inj = df.loc[Player[i]-1:idx.index.values.astype(int)[0]]\n",
    "            inj_df = pd.concat([inj_df,inj])\n",
    "\n",
    "        inj_df['Inj_Week'] = 0\n",
    "        inj_df['Inj_Week'] = inj_df.groupby('Player').cumcount()\n",
    "        inj_df.to_csv(f'../data/analysis-data/injury-db-{s}.csv',index=False)\n",
    "    end = time.time()\n",
    "    print(f'Injury databases created for seasons {Start} to {End}.',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# injured_database(2016,2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 4.3 - Split data before the player sustained an injury or for full season healthy players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def healthy_database(Start,End):\n",
    "    # create a database of all athletes that were healthy or healthy until sustaining an injury\n",
    "    start = time.time()\n",
    "    \n",
    "    season = []\n",
    "    for yr in range(Start,End+1):\n",
    "        season.append(yr)\n",
    "        \n",
    "    for s in season:\n",
    "        df = pd.read_csv(f'../data/clean-data/nfl-{s}.csv',low_memory=False)\n",
    "        df.sort_values(['Player','Week'],inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.drop(['index'],axis=1,inplace=True)\n",
    "        \n",
    "        # create list of status's to be able to identify were to split the rows\n",
    "        status = df['Status'].dropna().unique().tolist()\n",
    "        \n",
    "        # create a database to locate the index of the first instance of an injury\n",
    "        Player = df.loc[df['Status'].isin(status)].drop_duplicates(subset=['Player']).index.unique().tolist()\n",
    "        \n",
    "        # split out the rows then save them to a separate dataframe and export to csv\n",
    "        healthy_df = pd.DataFrame()\n",
    "        \n",
    "        for i in range(0,len(Player)):\n",
    "            idx = df[df['Player']==df['Player'][Player[i]]].head(1)\n",
    "            healthy = df.loc[idx.index.values.astype(int)[0]:Player[i]-1]\n",
    "            healthy_df = pd.concat([healthy_df,healthy])\n",
    "\n",
    "        healthy_df['Healthy_Week'] = 0\n",
    "        healthy_df['Healthy_Week'] = healthy_df.groupby('Player').cumcount()+1\n",
    "        healthy_df.to_csv(f'../data/analysis-data/healthy-db-{s}.csv',index=False)\n",
    "    end = time.time()\n",
    "    print(f'Healthy databases created for seasons {Start} to {End}.',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# healthy_database(2016,2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Dash App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Create UI From Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app(Start,End):\n",
    "    inj = pd.DataFrame()\n",
    "    heal = pd.DataFrame()\n",
    "\n",
    "    for s in range(Start,End+1):\n",
    "        i= pd.read_csv(f'../data/analysis-data/injury-db-{s}.csv',low_memory=False)\n",
    "        h = pd.read_csv(f'../data/analysis-data/healthy-db-{s}.csv',low_memory=False)\n",
    "        i['Date'] = pd.to_datetime(i['Date'],format='%Y-%m-%d')\n",
    "        h['Date'] = pd.to_datetime(h['Date'],format='%Y-%m-%d')\n",
    "        inj = pd.concat([inj,i])\n",
    "        heal = pd.concat([heal,h])\n",
    "\n",
    "    inj = inj[['Player','Pos','Team','Season','Fntsy_Pts','Snaps','Status','Injury','Inj_Week']]\n",
    "    inj = inj.query(\"Snaps >= 20 and Pos == 'RB'\")\n",
    "    # df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    inj = pd.pivot_table(inj,index=['Season','Injury'])\n",
    "    inj.reset_index(inplace=True)\n",
    "    inj.sort_values(\"Injury\",inplace=True)\n",
    "\n",
    "    heal = heal[['Player','Pos','Team','Season','Fntsy_Pts','Snaps','Healthy_Week']]\n",
    "    heal = heal.query(\"Snaps >= 20 and Pos == 'RB'\")\n",
    "    # df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    heal = pd.pivot_table(heal,index=['Season','Player'])\n",
    "    heal.reset_index(inplace=True)\n",
    "    heal.sort_values(\"Fntsy_Pts\",ascending=False,inplace=True)\n",
    "\n",
    "    app = JupyterDash(__name__)\n",
    "#     app.layout = html.Div([\n",
    "#             dash_table.DataTable(\n",
    "#                 id='heal_table',\n",
    "#                 columns=[{\n",
    "#                     'name':i,\n",
    "#                     'id':i,\n",
    "#                     'type':'numeric',\n",
    "#                     'format':Format(\n",
    "#                         scheme=Scheme.fixed,\n",
    "#                         precision=2,\n",
    "#                         decimal_delimiter='.',\n",
    "#                         trim=Trim.yes),\n",
    "#                     'selectable': True\n",
    "#                 } for i in heal.columns],\n",
    "#                 sort_mode = \"multi\",\n",
    "#                 sort_action = \"native\",\n",
    "#                 data=heal.to_dict('records'),\n",
    "#                 style_as_list_view = True,\n",
    "#                 style_cell = {\n",
    "#                     'padding':'5px',\n",
    "#                     'minWidth':'95px'\n",
    "#                 },\n",
    "#                 style_header = {\n",
    "#                     'backgroundColor':'#bababa',\n",
    "#                     'fontWeight':'bold'\n",
    "#                 },\n",
    "#                 style_cell_conditional = [\n",
    "#                     {\n",
    "#                         'if':{'column_id':c},\n",
    "#                         'textAlign':'left'\n",
    "#                     } for c in heal.columns\n",
    "#                 ],\n",
    "#                 style_data_conditional = (\n",
    "#                     [\n",
    "#                         {\n",
    "#                             'if':{\n",
    "#                                 'row_index':'odd'\n",
    "#                             },\n",
    "#                             'backgroundColor':'#f5f5f5'\n",
    "#                         }\n",
    "#                     ] + \n",
    "#                     [\n",
    "#                         {\n",
    "#                             'if':{\n",
    "#                                 'filter_query':'{{Fntsy_Pts}} = {}'.format(i),\n",
    "#                                 'column_id' : 'Fntsy_Pts'\n",
    "#                             },\n",
    "#                             'backgroundColor':'#14de1b'\n",
    "#                         }\n",
    "#                         for i in heal['Fntsy_Pts'].nlargest(5)\n",
    "#                     ] +\n",
    "#                     [\n",
    "#                         {\n",
    "#                             'if':{\n",
    "#                                 'filter_query':'{{{}}} <= {}'.format(col,val),\n",
    "#                                 'column_id': col\n",
    "#                             },\n",
    "#                             'backgroundColor':'red',\n",
    "#                             'color':'white'\n",
    "#                         }\n",
    "#                         for (col,val) in heal.quantile(0.1).iteritems()\n",
    "#                     ]\n",
    "#                 ),\n",
    "#                 fixed_rows = {'headers':True},\n",
    "#                 fixed_columns = {'headers':True,'data':1},\n",
    "#                 style_table = {\n",
    "#                     'height': 300,\n",
    "#                     'overflowY':'auto',\n",
    "#                     'overflowX':'auto',\n",
    "#                     'minWidth':'100%'\n",
    "#                 },\n",
    "#                 page_size = 25,\n",
    "#                 page_action = \"native\",\n",
    "#                 page_current = 0,\n",
    "#             ),\n",
    "#             html.Div(id='datatable-interactivity-container')])\n",
    "#     app.layout = html.Div([\n",
    "#             dash_table.DataTable(\n",
    "#                 id='inj_table',\n",
    "#                 columns=[{\n",
    "#                     'name':i,\n",
    "#                     'id':i,\n",
    "#                     'type':'numeric',\n",
    "#                     'format':Format(\n",
    "#                         scheme=Scheme.fixed,\n",
    "#                         precision=2,\n",
    "#                         decimal_delimiter='.',\n",
    "#                         trim=Trim.yes),\n",
    "#                     'selectable': True\n",
    "#                 } for i in inj.columns],\n",
    "#                 sort_mode = \"multi\",\n",
    "#                 sort_action = \"native\",\n",
    "#                 data=inj.to_dict('records'),\n",
    "#                 style_as_list_view = True,\n",
    "#                 style_cell = {\n",
    "#                     'padding':'5px',\n",
    "#                     'minWidth':'95px'\n",
    "#                 },\n",
    "#                 style_header = {\n",
    "#                     'backgroundColor':'#bababa',\n",
    "#                     'fontWeight':'bold'\n",
    "#                 },\n",
    "#                 style_cell_conditional = [\n",
    "#                     {\n",
    "#                         'if':{'column_id':c},\n",
    "#                         'textAlign':'left'\n",
    "#                     } for c in inj.columns\n",
    "#                 ],\n",
    "#                 style_data_conditional = (\n",
    "#                     [\n",
    "#                         {\n",
    "#                             'if':{\n",
    "#                                 'row_index':'odd'\n",
    "#                             },\n",
    "#                             'backgroundColor':'#f5f5f5'\n",
    "#                         }\n",
    "#                     ] + \n",
    "#                     [\n",
    "#                         {\n",
    "#                             'if':{\n",
    "#                                 'filter_query':'{{Fntsy_Pts}} = {}'.format(i),\n",
    "#                                 'column_id' : 'Fntsy_Pts'\n",
    "#                             },\n",
    "#                             'backgroundColor':'#14de1b'\n",
    "#                         }\n",
    "#                         for i in inj['Fntsy_Pts'].nlargest(5)\n",
    "#                     ] +\n",
    "#                     [\n",
    "#                         {\n",
    "#                             'if':{\n",
    "#                                 'filter_query':'{{{}}} <= {}'.format(col,val),\n",
    "#                                 'column_id': col\n",
    "#                             },\n",
    "#                             'backgroundColor':'red',\n",
    "#                             'color':'white'\n",
    "#                         }\n",
    "#                         for (col,val) in inj.quantile(0.1).iteritems()\n",
    "#                     ]\n",
    "#                 ),\n",
    "#                 fixed_rows = {'headers':True},\n",
    "#                 fixed_columns = {'headers':True,'data':1},\n",
    "#                 style_table = {\n",
    "#                     'height': 300,\n",
    "#                     'overflowY':'auto',\n",
    "#                     'overflowX':'auto',\n",
    "#                     'minWidth':'100%'\n",
    "#                 },\n",
    "#                 page_size = 25,\n",
    "#                 page_action = \"native\",\n",
    "#                 page_current = 0,\n",
    "#             ),\n",
    "#             html.Div(id='datatable-interactivity-container')])\n",
    "    app.run_server(mode='inline')#,port=8060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fantasy_socre(2016,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_app(2020,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start = 2016\n",
    "End = 2020\n",
    "inj = pd.DataFrame()\n",
    "heal = pd.DataFrame()\n",
    "\n",
    "for s in range(Start,End+1):\n",
    "    i= pd.read_csv(f'../data/analysis-data/injury-db-{s}.csv',low_memory=False)\n",
    "    h = pd.read_csv(f'../data/analysis-data/healthy-db-{s}.csv',low_memory=False)\n",
    "    i['Date'] = pd.to_datetime(i['Date'],format='%Y-%m-%d')\n",
    "    h['Date'] = pd.to_datetime(h['Date'],format='%Y-%m-%d')\n",
    "    inj = pd.concat([inj,i])\n",
    "    heal = pd.concat([heal,h])\n",
    "inj['Fntsy_Pts'] = ((inj['IPass_Yds'] * (1/25)) +\n",
    "                        (inj['IPass_TD'] * 4) +\n",
    "                        (inj['IPass_Int'] * -2) +\n",
    "                        (inj['IRush_Yds'] * (1/10)) +\n",
    "                        (inj['IRush_TD'] * 6) +\n",
    "                        (inj['IRec_Yds'] * (1/10)) +\n",
    "                        (inj['IRec_TD'] * 6) +\n",
    "                        (inj['IRec_Rec'] * 0.5) +\n",
    "                        (inj['I2pt_Made'] * 2) +\n",
    "                        (inj['IFmb_Lost'] * -2) +\n",
    "                        (inj['IKR_TD'] * 6) +\n",
    "                        (inj['IPR_TD'] * 6) +\n",
    "                        (inj['ITack_Sk'] * 3) +\n",
    "                        (inj['ITack_Solo'] * 1) +\n",
    "                        (inj['ITack_Ast'] * 0.5) +\n",
    "                        (inj['ITack_TFL'] * 1.5) +\n",
    "                        (inj['IDef_Int'] * 3) +\n",
    "                        (inj['IFmb_Forced'] * 3) +\n",
    "                        (inj['IFmb_Recov'] * 3) +\n",
    "                        ((inj['ITot_TD'] -\n",
    "                          inj['IPass_TD'] -\n",
    "                          inj['IRush_TD'] -\n",
    "                          inj['IRec_TD'] -\n",
    "                          inj['IKR_TD'] -\n",
    "                          inj['IPR_TD']) * 6) +\n",
    "                        (inj['ISfty'] * 2) +\n",
    "                        (inj['IDef_PD'] * 3))\n",
    "\n",
    "heal['Fntsy_Pts'] = ((heal['IPass_Yds'] * (1/25)) +\n",
    "                        (heal['IPass_TD'] * 4) +\n",
    "                        (heal['IPass_Int'] * -2) +\n",
    "                        (heal['IRush_Yds'] * (1/10)) +\n",
    "                        (heal['IRush_TD'] * 6) +\n",
    "                        (heal['IRec_Yds'] * (1/10)) +\n",
    "                        (heal['IRec_TD'] * 6) +\n",
    "                        (heal['IRec_Rec'] * 0.5) +\n",
    "                        (heal['I2pt_Made'] * 2) +\n",
    "                        (heal['IFmb_Lost'] * -2) +\n",
    "                        (heal['IKR_TD'] * 6) +\n",
    "                        (heal['IPR_TD'] * 6) +\n",
    "                        (heal['ITack_Sk'] * 3) +\n",
    "                        (heal['ITack_Solo'] * 1) +\n",
    "                        (heal['ITack_Ast'] * 0.5) +\n",
    "                        (heal['ITack_TFL'] * 1.5) +\n",
    "                        (heal['IDef_Int'] * 3) +\n",
    "                        (heal['IFmb_Forced'] * 3) +\n",
    "                        (heal['IFmb_Recov'] * 3) +\n",
    "                        ((heal['ITot_TD'] -\n",
    "                          heal['IPass_TD'] -\n",
    "                          heal['IRush_TD'] -\n",
    "                          heal['IRec_TD'] -\n",
    "                          heal['IKR_TD'] -\n",
    "                          heal['IPR_TD']) * 6) +\n",
    "                        (heal['ISfty'] * 2) +\n",
    "                        (heal['IDef_PD'] * 3))\n",
    "\n",
    "data = pd.concat([inj,heal])\n",
    "data = data[['Player','Pos','Team','Season','Week','Fntsy_Pts','Snaps','Status','Injury','Inj_Week','Healthy_Week']]\n",
    "\n",
    "inj = inj[['Player','Pos','Team','Season','Week','Fntsy_Pts','Snaps','Status','Injury','Inj_Week']]\n",
    "heal = heal[['Player','Pos','Team','Season','Week','Fntsy_Pts','Snaps','Healthy_Week']]\n",
    "\n",
    "piv_inj = pd.pivot_table(inj,index=['Season','Injury'])\n",
    "piv_inj.reset_index(inplace=True)\n",
    "piv_inj.sort_values(\"Injury\",inplace=True)\n",
    "\n",
    "piv_heal = pd.pivot_table(heal,index=['Season','Player'])\n",
    "piv_heal.reset_index(inplace=True)\n",
    "piv_heal.sort_values(\"Fntsy_Pts\",ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('Fntsy_Pts',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
