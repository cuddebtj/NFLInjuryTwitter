{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e63b1-15a3-45cf-9250-1b0582e999a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5152b-7268-4037-8ca1-2eead26bba30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f90430-a59d-45ef-b9ab-5d051f38e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_database():\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    p_database = p_database.lower()\n",
    "\n",
    "    abcd = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    player_url = \"https://www.pro-football-reference.com/players/{abcd}/\"\n",
    "    players = []\n",
    "    for c in range(0,len(abcd)):\n",
    "        res = requests.get(player_url.format(abcd=abcd[c]))\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        data = soup.find_all('p')\n",
    "\n",
    "        for i in data:\n",
    "            a = i.find('a')['href']\n",
    "            pl = i.get_text()[:(i.get_text().find(\"(\")-1)]\n",
    "            po = i.get_text()[(i.get_text().find(\"(\")+1):i.get_text().find(\")\")]\n",
    "            fy = i.get_text()[(i.get_text().rfind(\"-\")-4):(i.get_text().rfind(\"-\"))]\n",
    "            ly = i.get_text()[(i.get_text().rfind(\"-\")+1):(i.get_text().rfind(\"-\")+5)]\n",
    "            p = {\"Address\": a[:-4], \n",
    "                 \"Player\": pl, \n",
    "                 \"Position\": po, \n",
    "                 \"First_Year\":  fy, \n",
    "                 \"Last_Year\": ly}\n",
    "            players.append(p)\n",
    "\n",
    "    players_df = pd.DataFrame().from_dict(players)\n",
    "    players_df = players_df[players_df.Last_Year != \"Ever\"]\n",
    "    players_df['Player'] = players_df['Player'].str.replace('+','')\n",
    "    players_df['Last_Year'] = players_df.Last_Year.astype('int32')\n",
    "    players_df.to_csv('../data/database-players.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7749725-73c7-4e82-a423-047522563b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data(Season,url):\n",
    "    # scrape weekly team stats from www.stathead.com\n",
    "    start = time.time()\n",
    "    # page to start scrape at\n",
    "    page = 0\n",
    "    # login payload information pulled from a .env file\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    \n",
    "    # lots of team data, could not fit into one pull from stathead, needed to use 2 different url's, this allows the function to iterate through both\n",
    "    if url == 1:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=points&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=E&game_num_min=0&year_min={Season}&cstat[1]=all_td_team&ccomp[1]=gt&cval[1]=0&cstat[2]=third_down_att&ccomp[2]=gt&cval[2]=0&cstat[3]=vegas_line&ccomp[3]=gt&cval[3]=-50&cstat[4]=penalties&ccomp[4]=gt&cval[4]=0&cstat[5]=rush_att&ccomp[5]=gt&cval[5]=0&cstat[6]=tot_yds&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=pass_cmp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url == 2:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=all_td_opp&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={Season}&cstat[1]=tot_yds_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_yds_diff&ccomp[2]=gt&cval[2]=-500&cstat[3]=score_diff_thru_1&ccomp[3]=gt&cval[3]=-500&cstat[4]=rush_att_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=kick_ret_td_tgl&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp_opp&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down_opp&ccomp[7]=gt&cval[7]=0&cstat[8]=score_diff_1_qtr&ccomp[8]=gt&cval[8]=-500&cstat[9]=third_down_att_opp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url != 1 or 2:\n",
    "        print(\"Please select 1 or 2.\")\n",
    "    \n",
    "    # open logged in session for scraping\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        # beginning the scrape and stopping the scrape when page number reaches 100k\n",
    "        try:\n",
    "\n",
    "            while page < 100000:\n",
    "                \n",
    "                # pulling the website and scraping it\n",
    "                website = session.get(stat_url.format(Season=Season,page=page)).text\n",
    "                soup = BeautifulSoup(website, 'html')\n",
    "                table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "                \n",
    "                # pull headers and rows out of the data\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "                \n",
    "                # final location for complete data\n",
    "                final_data = []\n",
    "                \n",
    "                # create row for each line of data in table\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "                \n",
    "                # create the dataframe in panadas excluding the blank row and matching headers with the rows\n",
    "                df = pd.DataFrame(final_data[1:], columns=table_headers[12:])\n",
    "                \n",
    "                # writting dataframe to csv, continuous appending just incase the function fails, data will be saved\n",
    "                if url == 1:\n",
    "                    df.to_csv(f'../data/raw-data/nfl-team-data-{Season}-1-raw.csv',mode='a',index=False)\n",
    "                else:\n",
    "                    df.to_csv(f'../data/raw-data/nfl-team-data-{Season}-2-raw.csv',mode='a',index=False)\n",
    "                \n",
    "                # progress through the websites\n",
    "                page += 100\n",
    "            \n",
    "        except:\n",
    "            # notifying the scrape has completed.\n",
    "            end = time.time()\n",
    "            print(f'Done: Team {Season}, {url}, {page}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b58043-a101-468d-88c7-be63c868167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_snaps(Season):\n",
    "    players = pd.read_csv('../data/database-players.csv')\n",
    "\n",
    "    nfl_weeks = pd.read_csv('../data/NFL-Week-Dates.csv')\n",
    "\n",
    "    \n",
    "    \n",
    "    for p_add in players['Address'][2124:]:\n",
    "        \n",
    "        snap_counts_df = pd.DataFrame()\n",
    "        \n",
    "        for s in range(Season_Start, Season_End+1):\n",
    "            \n",
    "            df = pd.read_html(f'https://www.pro-football-reference.com{p_add}/fantasy/{s}', header=2, attrs={'id': 'player_fantasy'})\n",
    "            df = df[0]\n",
    "            df = df.head(-1)\n",
    "            \n",
    "            df['Player_Address'] = p_add\n",
    "            df['Season'] = s\n",
    "            df['Date'] = pd.to_datetime(df['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "            \n",
    "            nfl_weeks['Week'] = nfl_weeks['Week'].astype(str)\n",
    "\n",
    "            # create function to calculate what nfl week the date occured\n",
    "            def pre_thu(d):\n",
    "                days_behind = 3 - d.weekday()\n",
    "                if days_behind > 0:\n",
    "                    days_behind -= 7\n",
    "                return d + dt.timedelta(days_behind)\n",
    "            \n",
    "            df['week_start_nfl'] = df['Date'].apply(pre_thu)\n",
    "            \n",
    "            nfl_weeks['Start Date'] = pd.to_datetime(nfl_weeks['Start Date'])\n",
    "            \n",
    "            df = pd.merge(left=df,right=nfl_weeks,how='left',left_on='week_start_nfl',right_on='Start Date')\n",
    "\n",
    "            snap_counts_df = pd.concat([df, snap_counts_df])\n",
    "            \n",
    "#             time.sleep(5)\n",
    "            \n",
    "        snap_counts_df = snap_counts_df[['Player_Address', 'Pos', 'Date',\n",
    "                                         'Season', 'Week', 'G#', 'Tm', \n",
    "                                         'Opp', snap_counts_df.columns[4], \n",
    "                                         'Result', 'Num', 'Pct', 'Num.1', \n",
    "                                         'Pct.1', 'Num.2', 'Pct.2']]\n",
    "        snap_counts_df.columns = ['Player_Address', 'Pos', 'Date',\n",
    "                                  'Season', 'Week', 'G#', 'Team', \n",
    "                                  'Opp', 'Home_Away', 'Result',  \n",
    "                                  'Off_Snaps', 'Off_Snaps%', 'Def_Snaps', \n",
    "                                  'Def_Snaps%', 'ST_Snaps', 'ST_Snaps%']\n",
    "        snap_counts_df.replace({'Home_Away': {'@': 'Away', None: 'Home'}},regex=True,inplace=True)\n",
    "        snap_counts_df[['Off_Snaps%', 'Def_Snaps%', 'ST_Snaps%']] = snap_counts_df[['Off_Snaps%', 'Def_Snaps%', 'ST_Snaps%']].astype(str)\n",
    "        snap_counts_df['Off_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['Off_Snaps%'].values))\n",
    "        snap_counts_df['Def_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['Def_Snaps%'].values))\n",
    "        snap_counts_df['ST_Snaps%'] = list(map(lambda x: x[:-1], snap_counts_df['ST_Snaps%'].values))\n",
    "        snap_counts_df.to_csv(\"../data/raw-data/snap-counts-raw.csv\", mode=\"a\", index=False)\n",
    "        \n",
    "        #time.sleep(900)\n",
    "            \n",
    "    end = time.time()\n",
    "    print(f'Done: Player Snaps from {Season_Start} to {Season_End}', f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca0053-d6ff-46f7-b894-8a8017c36d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730505c-b6ef-4ed7-b9de-b0376feabd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ddab27-6e66-430a-9681-f91983ef2ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c789c-dcc7-4f22-bd72-6d7a6328fd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1eb77c-d7ef-4f75-b954-a5564cac2e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702af0e-6e00-406d-9790-228d084151a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a338c5-0ac9-4fe1-bbdc-dc362dfefced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e959778-668d-4600-9915-01a4b2099538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddbafd-4bf6-4985-9746-b88d122b1066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0126c6-f6ed-4f58-a43e-333e49e1c144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cc156-918d-4c8f-a26c-10ec855503cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
