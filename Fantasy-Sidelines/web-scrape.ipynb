{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9e63b1-15a3-45cf-9250-1b0582e999a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f90430-a59d-45ef-b9ab-5d051f38e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_database():\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    p_database = p_database.lower()\n",
    "\n",
    "    abcd = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    player_url = \"https://www.pro-football-reference.com/players/{abcd}/\"\n",
    "    players = []\n",
    "    for c in range(0,len(abcd)):\n",
    "        res = requests.get(player_url.format(abcd=abcd[c]))\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        data = soup.find_all('p')\n",
    "\n",
    "        for i in data:\n",
    "            a = i.find('a')['href']\n",
    "            pl = i.get_text()[:(i.get_text().find(\"(\")-1)]\n",
    "            po = i.get_text()[(i.get_text().find(\"(\")+1):i.get_text().find(\")\")]\n",
    "            fy = i.get_text()[(i.get_text().rfind(\"-\")-4):(i.get_text().rfind(\"-\"))]\n",
    "            ly = i.get_text()[(i.get_text().rfind(\"-\")+1):(i.get_text().rfind(\"-\")+5)]\n",
    "            p = {\"Address\": a[:-4], \n",
    "                 \"Player\": pl, \n",
    "                 \"Position\": po, \n",
    "                 \"First_Year\":  fy, \n",
    "                 \"Last_Year\": ly}\n",
    "            players.append(p)\n",
    "\n",
    "    players_df = pd.DataFrame().from_dict(players)\n",
    "    players_df = players_df[players_df.Last_Year != \"Ever\"]\n",
    "    players_df['Player'] = players_df['Player'].str.replace('+','')\n",
    "    players_df['Last_Year'] = players_df.Last_Year.astype('int32')\n",
    "    players_df.to_csv('../data/database-players.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7749725-73c7-4e82-a423-047522563b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data(s,url):\n",
    "    # scrape weekly team stats from www.stathead.com\n",
    "    start = time.time()\n",
    "    # page to start scrape at\n",
    "    page = 0\n",
    "    # login payload information pulled from a .env file\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    \n",
    "    # lots of team data, could not fit into one pull from stathead, needed to use 2 different url's, this allows the function to iterate through both\n",
    "    if url == 1:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=points&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=E&game_num_min=0&year_min={Season}&cstat[1]=all_td_team&ccomp[1]=gt&cval[1]=0&cstat[2]=third_down_att&ccomp[2]=gt&cval[2]=0&cstat[3]=vegas_line&ccomp[3]=gt&cval[3]=-50&cstat[4]=penalties&ccomp[4]=gt&cval[4]=0&cstat[5]=rush_att&ccomp[5]=gt&cval[5]=0&cstat[6]=tot_yds&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=pass_cmp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url == 2:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=all_td_opp&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={Season}&cstat[1]=tot_yds_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_yds_diff&ccomp[2]=gt&cval[2]=-500&cstat[3]=score_diff_thru_1&ccomp[3]=gt&cval[3]=-500&cstat[4]=rush_att_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=kick_ret_td_tgl&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp_opp&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down_opp&ccomp[7]=gt&cval[7]=0&cstat[8]=score_diff_1_qtr&ccomp[8]=gt&cval[8]=-500&cstat[9]=third_down_att_opp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url != 1 or 2:\n",
    "        print(\"Please select 1 or 2.\")\n",
    "    \n",
    "    # open logged in session for scraping\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        # beginning the scrape and stopping the scrape when page number reaches 100k\n",
    "        try:\n",
    "\n",
    "            while page < 100000:\n",
    "                \n",
    "                # pulling the website and scraping it\n",
    "                website = session.get(stat_url.format(Season=s,page=page)).text\n",
    "                soup = BeautifulSoup(website, 'html')\n",
    "                table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "                \n",
    "                # pull headers and rows out of the data\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "                \n",
    "                # final location for complete data\n",
    "                final_data = []\n",
    "                \n",
    "                # create row for each line of data in table\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "                \n",
    "                # create the dataframe in panadas excluding the blank row and matching headers with the rows\n",
    "                df = pd.DataFrame(final_data[1:], columns=table_headers[12:])\n",
    "                \n",
    "                # writting dataframe to csv, continuous appending just incase the function fails, data will be saved\n",
    "                if url == 1:\n",
    "                    df.to_csv(f'../data/raw-data/nfl-team-data-{s}-1-raw.csv',mode='a',index=False)\n",
    "                else:\n",
    "                    df.to_csv(f'../data/raw-data/nfl-team-data-{s}-2-raw.csv',mode='a',index=False)\n",
    "                \n",
    "                # progress through the websites\n",
    "                page += 100\n",
    "            \n",
    "        except:\n",
    "            # notifying the scrape has completed.\n",
    "            end = time.time()\n",
    "            print(f'Done: Team {s}, {url}, {page}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca0053-d6ff-46f7-b894-8a8017c36d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def injury_reports(s):\n",
    "\n",
    "    # scrape weekly nfl injury reports\n",
    "    start = time.time()\n",
    "    # list of team abbreviations from pro-football-reference for url purposes\n",
    "    teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "             'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "\n",
    "    ENDPOINT = 'https://www.pro-football-reference.com/teams/{team}/{Season}_injuries.htm'\n",
    "    \n",
    "    # creating final place to store data\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    # scrape pages for all teams\n",
    "    for team in teams:\n",
    "        \n",
    "        # open webpage and scrape contents into lists, then to a dataframe\n",
    "        res = requests.get(ENDPOINT.format(Season=s,team=team))\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "        table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "        table_rows = table.find_all('tr')\n",
    "\n",
    "        final_data = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all(['th','td'])\n",
    "            row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "            final_data.append(row)\n",
    "\n",
    "        df_data = final_data[1:]\n",
    "        data_body = [[df_data[j][i] for j in range(len(df_data))] for i in range(len(df_data[0]))]\n",
    "\n",
    "        df = pd.DataFrame(data_body,final_data[0]).T\n",
    "        # adding team and season columns for identification\n",
    "        df.insert(loc=1,column='Team',value=team)\n",
    "        df.insert(loc=2,column='Season',value=s)\n",
    "\n",
    "        # combine current data with final dataframe\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # rename column\n",
    "    final_df.rename(columns={'PlayerÂ ':'Player'},inplace=True)\n",
    "    \n",
    "    # write final data to csv file\n",
    "    final_df.to_csv(f'../data/raw-data/nfl-injury-report-{s}-raw.csv',index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Injury Reports {s}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730505c-b6ef-4ed7-b9de-b0376feabd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_stats(s):\n",
    "    start = time.time()\n",
    "    players = pd.read_csv('../data/database-players.csv')\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for p_add in players['Address'][2000:2020]:\n",
    "\n",
    "        try:\n",
    "            p_id = p_add[11:]\n",
    "            df = pd.read_html(f'https://www.pro-football-reference.com{p_add}/gamelog/{s}', header=[0,1], attrs={'id': 'stats'})\n",
    "            df = df[0]\n",
    "\n",
    "            df['Player_Address'] = p_add\n",
    "            df['Season'] = s\n",
    "\n",
    "            final_df = pd.concat([df, final_df])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    final_df.to_csv(f'../data/data-raw/player-stats-{s}.csv', index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Player stats scrape (including snap counts) for {} season',f'{(end-start)/60}: minutes to complete.', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ddab27-6e66-430a-9681-f91983ef2ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c789c-dcc7-4f22-bd72-6d7a6328fd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1eb77c-d7ef-4f75-b954-a5564cac2e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702af0e-6e00-406d-9790-228d084151a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a338c5-0ac9-4fe1-bbdc-dc362dfefced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e959778-668d-4600-9915-01a4b2099538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddbafd-4bf6-4985-9746-b88d122b1066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0126c6-f6ed-4f58-a43e-333e49e1c144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cc156-918d-4c8f-a26c-10ec855503cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
