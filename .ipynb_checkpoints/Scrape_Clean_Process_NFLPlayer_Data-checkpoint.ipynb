{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def team_snaps_scrape(Start_Year,End_Year):\n",
    "    page = 0\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    stat_url = f\"https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=vegas_line&match=game&year_max={str(End_Year)}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={str(Start_Year)}&offset=\"\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        \n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        while page < 10000:\n",
    "            \n",
    "            page1 = str(page)\n",
    "            website = session.get(stat_url+page1).text\n",
    "            soup = BeautifulSoup(website, 'html')\n",
    "            table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "\n",
    "            table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "            table_rows = table.find_all('tr')\n",
    "\n",
    "            final_data = []\n",
    "            \n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all('td')\n",
    "                row = [tr.text for tr in td]\n",
    "                final_data.append(row)\n",
    "                \n",
    "            df = pd.DataFrame(final_data, columns=table_headers[3:])\n",
    "            df.to_csv(f'nflteamsnaps_{Start_Year}_{End_Year}.csv',index=False)\n",
    "            page += 100\n",
    "            print(page)\n",
    "\n",
    "def player_snap_scrape(Start_Year,End_Year):\n",
    "    \n",
    "    ENDPOINT = \"https://www.fantasypros.com/nfl/reports/snap-counts/?year={year}\"\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for year in range(Start_Year, End_Year+1):\n",
    "        res = requests.get(ENDPOINT.format(year=year))\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        df.columns = df.columns[:3].tolist() + [f'Week {i}' for i in df.columns[3:-2]] + df.columns[-2:].tolist()\n",
    "\n",
    "        df['Year'] = year\n",
    "\n",
    "        cols = df.columns[:3].tolist() + df.columns[-1:].tolist() + df.columns[3:-1].tolist()\n",
    "        df = df[cols]\n",
    "\n",
    "        final_df = pd.concat([final_df, df])\n",
    "\n",
    "    final_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\snapcounts_{Start_Year}_{End_Year}.csv')\n",
    "    print('Done')\n",
    "\n",
    "def player_snap_clean(Start_Year,End_Year):\n",
    "    \n",
    "    dfname = []\n",
    "    \n",
    "    for num in range(Start_Year,End_Year+1):\n",
    "        dfname.append(f'{num}_Offense')\n",
    "        dfname.append(f'{num}_Defense')\n",
    "        data = {key: pd.read_csv(f'FantasyPros_Fantasy_Football_{key}_Snap_Counts.csv') for key in dfname}\n",
    "\n",
    "    yr = Start_Year\n",
    "        \n",
    "    while yr < End_Year+1:\n",
    "        for key in data:\n",
    "            data[key].insert(loc=3,column='Year',value=yr,allow_duplicates=True)\n",
    "            data[key]['Year'] = data[key]['Year'].astype(int)\n",
    "            yr += 0.5\n",
    "\n",
    "    for key in data:\n",
    "        data[key] = pd.melt(data[key],id_vars=['Player','Pos','Team','Year'],var_name='Week', value_name='Snaps')\n",
    "        data[key].replace({'Team':\\\n",
    "                  {'GB':'GNB','JAC':'JAX','KC':'KAN','NE':'NWE','NO':'NOR','SF':\\\n",
    "                   'SFO','TB':'TAM','Multi':None,'LV':'OAK',},'Snaps':{'bye':None}},inplace=True)\n",
    "        data[key].dropna(thresh=3,inplace=True)\n",
    "\n",
    "    player = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "    team = pd.read_csv(f'nflteamsnaps_{Start_Year}_{End_Year}.csv')\n",
    "    team.dropna(thresh=5,inplace=True)\n",
    "    team.drop(['LTime','Y/P'],axis=1,inplace=True)\n",
    "    team.rename(columns={'Unnamed: 5':'Away_Home','Time.1':'ToG','Tm':'Team'},inplace=True)\n",
    "    team['Date'] = pd.to_datetime(team['Date'])\n",
    "    team['ToG'] = pd.to_datetime(team['ToG'])\n",
    "    team['Away_Home'].fillna(value='Home',inplace=True)\n",
    "    team['Away_Home'].replace('@','Away',inplace=True)\n",
    "    team['Away_Home'].replace('nan','Home',inplace=True)\n",
    "    team['TO'].fillna(value=0,inplace=True)\n",
    "    team.insert(loc=2,column='Month',value=team['Date'].dt.month)\n",
    "    df_bridge = team['ToP'].str.split(\":\",expand=True)\n",
    "    team['ToP'] = (df_bridge[0].astype(int)*60)+df_bridge[1].astype(int)\n",
    "    team['Week'] = team['Week'].astype(str)\n",
    "\n",
    "    player_snaps = pd.merge(left=player,right=team,how='outer',left_on=['Team','Year','Week'],right_on=['Team','Year','Week'])\n",
    "    \n",
    "    player_snaps.to_csv(f'player_snaps_{Start_Year}_{End_Year}.csv',index=False)\n",
    "    print(\"Done\")\n",
    "\n",
    "def injury_reports_scrape(Start_Year,End_Year):\n",
    "    \n",
    "    #list of the NFL teams for url purposes\n",
    "    teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "             'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "    years = []\n",
    "    \n",
    "    #list of years to pull data for url purposes\n",
    "    for yr in range(Start_Year,End_Year+1):\n",
    "        years.append(yr)\n",
    "\n",
    "    #starting points to iterate through\n",
    "    team = 0\n",
    "    year = 0\n",
    "    dfname = []\n",
    "    \n",
    "    while team < 32:\n",
    "        \n",
    "        dfname.append(f'{teams[team]}_{years[year]}_injuryreport')\n",
    "        \n",
    "        #url for web scraping\n",
    "        url = f'https://www.pro-football-reference.com/teams/{teams[team]}/{years[year]}_injuries.htm'\n",
    "        \n",
    "        #opening website\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "        \n",
    "        #finding table\n",
    "        table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "        table_rows = table.find_all('tr')\n",
    "\n",
    "        #scraping the data\n",
    "        final_data = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all(['th','td'])\n",
    "            row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "            final_data.append(row)\n",
    "\n",
    "        #creatingdataframe to save\n",
    "        dfdata = final_data[1:]\n",
    "        data_body = [[dfdata[j][i] for j in range(len(dfdata))] for i in range(len(dfdata[0]))]\n",
    "        data = {key: pd.DataFrame(data_body,final_data[0]).T for key in dfname}\n",
    "        \n",
    "        key = f'{teams[team]}_{years[year]}_injuryreport'\n",
    "        \n",
    "        data[key].insert(loc=1,column='Team',value=teams[team],allow_duplicates=True)\n",
    "        data[key].insert(loc=2,column='Year',value=years[year],allow_duplicates=True)\n",
    "        data[key].to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\injury_reports\\{team}_{year}_injuryreport.csv',index=True)\n",
    "\n",
    "        #advancing through url\n",
    "        if year < len(years)-1:\n",
    "            year += 1\n",
    "\n",
    "        else:\n",
    "            year = 0\n",
    "            team += 1\n",
    "    print('Done')\n",
    "\n",
    "def injury_reports_clean(Start_Year,End_Year):\n",
    "    dfname = []\n",
    "    teams = ['crd','atl','rav','buf','car','chi','cin','cle','dal','den','det','gnb','htx','clt','jax','kan',\n",
    "                     'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "\n",
    "    for team in teams:\n",
    "        for year in range(Start_Year,End_Year+1):\n",
    "            dfname.append(f'{team}_{year}_injuryreport')\n",
    "            data = {key: pd.read_csv(f'{key}.csv') for key in dfname}\n",
    "\n",
    "    for key in data:\n",
    "        data[key].drop('Unnamed: 0',axis=1,inplace=True)\n",
    "        data[key] = pd.melt(data[key],id_vars=['Player','Team','Year'],var_name='Date', value_name='Status')\n",
    "        data[key][['Date','Opp']] = data[key].Date.str.split(\"vs. \",expand=True)\n",
    "        data[key][['Status','Injury']] = data[key].Status.str.split(\":\",expand=True)\n",
    "        data[key]['Date'] = data[key]['Date'].astype(str)+'/'+data[key]['Year'].astype(str)\n",
    "        data[key]['Date'] = pd.to_datetime(data[key]['Date'])\n",
    "        data[key].replace({'Team':\\\n",
    "                           {'crd':'ARI', 'atl':'ATL', 'rav':'BAL', 'buf':'BUF', 'car':'CAR', 'chi':'CHI', 'cin':'CIN',\\\n",
    "                            'cle':'CLE', 'dal':'DAL', 'den':'DEN', 'det':'DET', 'gnb':'GNB','htx':'HOU','clt':'IND',\\\n",
    "                            'jax':'JAX','kan':'KAN','sdg':'LAC','ram':'LAR','mia':'MIA','min':'MIN','nor':'NOR','nwe':'NWE',\\\n",
    "                            'nyg':'NYG','nyj':'NYJ','rai':'OAK','phi':'PHI','pit':'PIT','sea':'SEA','sfo':'SFO','tam':'TAM',\\\n",
    "                            'oti':'TEN','was':'WAS'}},inplace=True)\n",
    "        data[key].dropna(thresh=3,inplace=True)\n",
    "\n",
    "    nfl_injury = pd.concat(data.values(),ignore_index=True)\n",
    "    nfl_injury.to_csv(f'NFL_{Start_Year}_{End_Year}_Injuryreport.csv',index=False)\n",
    "    print('Done')\n",
    "\n",
    "def player_stats_scape(Start_Year,End_Year):\n",
    "    \n",
    "    pro_login_url = \"https://secure.fantasypros.com/accounts/login/\"\n",
    "    pro_user_name = os.environ.get('fantasyprosusername ')\n",
    "    pro_password = os.environ.get('fantasyprospassword ')\n",
    "    pro_payload = {\n",
    "        'username': pro_user_name,\n",
    "        'password': pro_password\n",
    "    }\n",
    "    \n",
    "    position = ['qb','rb','wr','te','dl','lb','db']\n",
    "    weeks = []\n",
    "    years = []\n",
    "    dfname = []\n",
    "\n",
    "    for wk in range(1,2):\n",
    "        weeks.append(wk)\n",
    "\n",
    "    for yr in range(2017,2018):\n",
    "        years.append(yr)\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        \n",
    "        s = session.post(pro_login_url, data=pro_payload)\n",
    "        \n",
    "        for pos in position:\n",
    "            for week in weeks:\n",
    "                for year in years:\n",
    "                    \n",
    "                    dfname.append(f'player_stats_{pos}_{year}_{week}_scrape')\n",
    "                    \n",
    "                    url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?league=3836944&year={year}&week={week}&range=week\"\n",
    "            \n",
    "                    r = requests.get(url)\n",
    "                    soup = BeautifulSoup(r.content, 'html')\n",
    "\n",
    "                    table = soup.find('table', attrs={'id': 'data','class': 'table'})\n",
    "                    table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                    table_rows = table.find_all('tr')\n",
    "\n",
    "                    final_data = []\n",
    "\n",
    "                    for tr in table_rows:\n",
    "                        td = tr.find_all('td')\n",
    "                        row = [tr.text for tr in td]\n",
    "                        final_data.append(row)\n",
    "\n",
    "                    data = {key: pd.DataFrame(final_data[1:], columns=table_headers) for key in dfname}\n",
    "\n",
    "                    key = f'player_stats_{pos}_{year}_{week}_scrape'\n",
    "\n",
    "                    #data[key].to_csv(f'player_stats_{position[pos]}_{years[year]}_{weeks[week]}.csv',index=False)\n",
    "                    print(key)\n",
    "        print(data)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start = 2017\n",
    "End = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#team_snaps_scrape(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"player_snap_scrape(Start,End)\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "injury_reports_scrape(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#player_snap_clean(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#injury_reports_clean(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#player_stats_scape(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = \"https://www.fantasypros.com/nfl/reports/snap-counts/?year={year}\"\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for year in range(Start_Year, End_Year+1):\n",
    "    res = requests.get(ENDPOINT.format(year=year))\n",
    "\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "    df = pd.read_html(str(table))[0]\n",
    "\n",
    "    df.columns = df.columns[:3].tolist() + [f'Week {i}' for i in df.columns[3:-2]] + df.columns[-2:].tolist()\n",
    "\n",
    "    df['Year'] = year\n",
    "\n",
    "    cols = df.columns[:3].tolist() + df.columns[-1:].tolist() + df.columns[3:-1].tolist()\n",
    "    df = df[cols]\n",
    "\n",
    "    final_df = pd.concat([final_df, df])\n",
    "\n",
    "final_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\snapcounts_{Start_Year}_{End_Year}.csv')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "         'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "ENDPOINT = 'https://www.pro-football-reference.com/teams/{team}/{year}_injuries.htm'\n",
    "\n",
    "for year in range(Start_Year, End_Year+1):\n",
    "    for team in teams:\n",
    "        res = requests.get(ENDPOINT.format(year=year,team=team))\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "        table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "        table_rows = table.find_all('tr')\n",
    "\n",
    "        final_data = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all(['th','td'])\n",
    "            row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "            final_data.append(row)\n",
    "\n",
    "        dfdata = final_data[1:]\n",
    "        data_body = [[dfdata[j][i] for j in range(len(dfdata))] for i in range(len(dfdata[0]))]\n",
    "        data = {key: pd.DataFrame(data_body,final_data[0]).T for key in dfname}\n",
    "\n",
    "        key = f'{teams[team]}_{years[year]}_injuryreport'\n",
    "\n",
    "        data[key].insert(loc=1,column='Team',value=teams[team],allow_duplicates=True)\n",
    "        data[key].insert(loc=2,column='Year',value=years[year],allow_duplicates=True)\n",
    "        data[key].to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\injury_reports\\{team}_{year}_injuryreport.csv',index=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
