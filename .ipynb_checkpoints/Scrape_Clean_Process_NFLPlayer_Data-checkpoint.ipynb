{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def team_data_scrape(Start_Year,End_Year,url):\n",
    "    page = 0\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    \n",
    "    if url == 1:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=points&match=game&year_max={End_Year}&order_by_asc=0&week_num_min=0&game_type=E&game_num_min=0&year_min={Start_Year}&cstat[1]=all_td_team&ccomp[1]=gt&cval[1]=0&cstat[2]=third_down_att&ccomp[2]=gt&cval[2]=0&cstat[3]=vegas_line&ccomp[3]=gt&cval[3]=-50&cstat[4]=penalties&ccomp[4]=gt&cval[4]=0&cstat[5]=rush_att&ccomp[5]=gt&cval[5]=0&cstat[6]=tot_yds&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=pass_cmp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url == 2:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=all_td_opp&match=game&year_max={End_Year}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={Start_Year}&cstat[1]=tot_yds_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_yds_diff&ccomp[2]=gt&cval[2]=-500&cstat[3]=score_diff_thru_1&ccomp[3]=gt&cval[3]=-500&cstat[4]=rush_att_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=kick_ret_td_tgl&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp_opp&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down_opp&ccomp[7]=gt&cval[7]=0&cstat[8]=score_diff_1_qtr&ccomp[8]=gt&cval[8]=-500&cstat[9]=third_down_att_opp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url != 1 or 2:\n",
    "        print(\"Please select 1 or 2.\")\n",
    "        \n",
    "    with requests.Session() as session:\n",
    "        \n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        while page < 100000:\n",
    "            \n",
    "            website = session.get(stat_url.format(Start_Year=Start_Year,End_Year=End_Year,page=page)).text\n",
    "            soup = BeautifulSoup(website, 'html')\n",
    "            table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "\n",
    "            table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "            table_rows = table.find_all('tr')\n",
    "\n",
    "            final_data = []\n",
    "            \n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all('td')\n",
    "                row = [tr.text for tr in td]\n",
    "                final_data.append(row)\n",
    "                \n",
    "            df = pd.DataFrame(final_data[1:], columns=table_headers[12:])\n",
    "            \n",
    "            print(page)\n",
    "            \n",
    "            if url == 1:\n",
    "                df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\nflteam_data_1_{Start_Year}_{End_Year}.csv',mode='a',index=False)\n",
    "            else:\n",
    "                df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\nflteam_data_2_{Start_Year}_{End_Year}.csv',mode='a',index=False)\n",
    "\n",
    "            page += 100\n",
    "                \n",
    "        print('Done')\n",
    "        print(page)\n",
    "\n",
    "def player_snap_scrape(Start_Year,End_Year):\n",
    "    \n",
    "    sides = ['','defense.php']\n",
    "    \n",
    "    ENDPOINT = \"https://www.fantasypros.com/nfl/reports/snap-counts/{side}?year={year}\"\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for year in range(Start_Year, End_Year+1):\n",
    "        \n",
    "        for side in sides:\n",
    "        \n",
    "            res = requests.get(ENDPOINT.format(year=year,side=side))\n",
    "\n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "            table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "            df = pd.read_html(str(table))[0]\n",
    "\n",
    "            df.columns = df.columns[:3].tolist() + [f'Week {i}' for i in df.columns[3:-2]] + df.columns[-2:].tolist()\n",
    "\n",
    "            df['Year'] = year\n",
    "\n",
    "            cols = df.columns[:3].tolist() + df.columns[-1:].tolist() + df.columns[3:-1].tolist()\n",
    "            df = df[cols]\n",
    "\n",
    "            final_df = pd.concat([final_df, df])\n",
    "            print(year,side)\n",
    "\n",
    "    final_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\snapcounts_{Start_Year}_{End_Year}.csv',index=False)\n",
    "    print('Done')\n",
    "\n",
    "def injury_reports_scrape(Start_Year,End_Year):\n",
    "    \n",
    "    teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "             'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "\n",
    "    ENDPOINT = 'https://www.pro-football-reference.com/teams/{team}/{year}_injuries.htm'\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for year in range(Start_Year, End_Year+1):\n",
    "\n",
    "        for team in teams:\n",
    "\n",
    "            res = requests.get(ENDPOINT.format(year=year,team=team))\n",
    "\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "            table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "            table_rows = table.find_all('tr')\n",
    "\n",
    "            final_data = []\n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all(['th','td'])\n",
    "                row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "                final_data.append(row)\n",
    "\n",
    "            dfdata = final_data[1:]\n",
    "            data_body = [[dfdata[j][i] for j in range(len(dfdata))] for i in range(len(dfdata[0]))]\n",
    "\n",
    "            df = pd.DataFrame(data_body,final_data[0]).T\n",
    "\n",
    "            df.insert(loc=1,column='Team',value=team)\n",
    "            df.insert(loc=2,column='Year',value=year)\n",
    "\n",
    "            final_df = pd.concat([final_df, df])\n",
    "            print(team,year)\n",
    "\n",
    "    final_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\injury_reports\\nlf_injuryreport_{Start_Year}_{End_Year}.csv',index=False)\n",
    "    print('Done')    \n",
    "\n",
    "def player_stats_scape(Start_Year,End_Year):\n",
    "    page = 0\n",
    "    location = 5000\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    stat_url = \"https://stathead.com/football/pgl_finder.cgi?request=1&game_num_max=99&week_num_max=99&order_by=all_td&season_start=1&qb_gwd=0&order_by_asc=0&qb_comeback=0&week_num_min=0&game_num_min=0&year_min={Start_Year}&match=game&year_max={End_Year}&season_end=-1&age_min=0&game_type=R&age_max=99&positions[]=qb&positions[]=rb&positions[]=wr&positions[]=te&positions[]=e&positions[]=t&positions[]=g&positions[]=c&positions[]=ol&positions[]=dt&positions[]=de&positions[]=dl&positions[]=ilb&positions[]=olb&positions[]=lb&positions[]=cb&positions[]=s&positions[]=db&positions[]=k&positions[]=p&cstat[1]=punt_ret&ccomp[1]=gt&cval[1]=0&cstat[2]=sacks&ccomp[2]=gt&cval[2]=0&cstat[3]=fumbles&ccomp[3]=gt&cval[3]=0&cstat[4]=rush_att&ccomp[4]=gt&cval[4]=0&cstat[5]=pass_defended&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp&ccomp[6]=gt&cval[6]=0&cstat[7]=targets&ccomp[7]=gt&cval[7]=0&cstat[8]=kick_ret&ccomp[8]=gt&cval[8]=0&offset={page}\"\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        \n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        while page < 100000:\n",
    "            \n",
    "            website = session.get(stat_url.format(Start_Year=Start_Year,End_Year=End_Year,page=page)).text\n",
    "            soup = BeautifulSoup(website, 'html')\n",
    "            table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "\n",
    "            table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "            table_rows = table.find_all('tr')\n",
    "\n",
    "            final_data = []\n",
    "            \n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all('td')\n",
    "                row = [tr.text for tr in td]\n",
    "                final_data.append(row)\n",
    "                \n",
    "            df = pd.DataFrame(final_data[1:], columns=table_headers[11:])\n",
    "            df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\player_stats\\player_stats_{Start_Year}_{End_Year}.csv',mode='a',index=False)\n",
    "\n",
    "            if page > location:\n",
    "                print(page)\n",
    "                location += 5000\n",
    "                \n",
    "            page += 100\n",
    "            \n",
    "        print('Done')\n",
    "        print(page)\n",
    "\n",
    "def player_snap_stats_clean(Start_Year,End_Year):\n",
    "    \n",
    "    snaps = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\snapcounts_{Start_Year}_{End_Year}.csv')\n",
    "    snaps.drop(['Unnamed: 0','TTL','AVG'],axis=1,inplace=True)\n",
    "    snaps = pd.melt(snaps, id_vars=['Player','Pos','Team','Year'], var_name='Week', value_name='Snaps')\n",
    "    snaps['Year'] = snaps['Year'].astype(str)\n",
    "    snaps.replace({'Team':{'FA':'','GB':'GNB','JAC':'JAX','KC':'KAN','Multi':'','NE':'NWE','NO':'NOR','SF':'SFO','TB':'TAM'},\\\n",
    "                    'Week':{'Week 1':'1','Week 2':'2','Week 3':'3','Week 4':'4','Week 5':'5','Week 6':'6',\\\n",
    "                            'Week 7':'7','Week 8':'8','Week 9':'9','Week 10':'10','Week 11':'11','Week 12':'12',\\\n",
    "                            'Week 13':'13','Week 14':'14','Week 15':'15','Week 16':'16','Week 17':'17'}},inplace=True)\n",
    "    \n",
    "    stats = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\player_stats\\player_stats_{Start_Year}_{End_Year}.csv')\n",
    "    \n",
    "    combined = pd.merge(left=stats,right=snaps,how='outer',left_on=['Team','Year','Week'],right_on=['Team','Year','Week'])\n",
    "    \n",
    "    combined.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\player_team_snaps_{Start_Year}_{End_Year}.csv',index=False)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "def injury_reports_clean(Start_Year,End_Year):\n",
    "    dfname = []\n",
    "    teams = ['crd','atl','rav','buf','car','chi','cin','cle','dal','den','det','gnb','htx','clt','jax','kan',\n",
    "                     'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "\n",
    "    for team in teams:\n",
    "        for year in range(Start_Year,End_Year+1):\n",
    "            dfname.append(f'{team}_{year}_injuryreport')\n",
    "            data = {key: pd.read_csv(f'{key}.csv') for key in dfname}\n",
    "\n",
    "    for key in data:\n",
    "        data[key].drop('Unnamed: 0',axis=1,inplace=True)\n",
    "        data[key] = pd.melt(data[key],id_vars=['Player','Team','Year'],var_name='Date', value_name='Status')\n",
    "        data[key][['Date','Opp']] = data[key].Date.str.split(\"vs. \",expand=True)\n",
    "        data[key][['Status','Injury']] = data[key].Status.str.split(\":\",expand=True)\n",
    "        data[key]['Date'] = data[key]['Date'].astype(str)+'/'+data[key]['Year'].astype(str)\n",
    "        data[key]['Date'] = pd.to_datetime(data[key]['Date'])\n",
    "        data[key].replace({'Team':\\\n",
    "                           {'crd':'ARI', 'atl':'ATL', 'rav':'BAL', 'buf':'BUF', 'car':'CAR', 'chi':'CHI', 'cin':'CIN',\\\n",
    "                            'cle':'CLE', 'dal':'DAL', 'den':'DEN', 'det':'DET', 'gnb':'GNB','htx':'HOU','clt':'IND',\\\n",
    "                            'jax':'JAX','kan':'KAN','sdg':'LAC','ram':'LAR','mia':'MIA','min':'MIN','nor':'NOR','nwe':'NWE',\\\n",
    "                            'nyg':'NYG','nyj':'NYJ','rai':'OAK','phi':'PHI','pit':'PIT','sea':'SEA','sfo':'SFO','tam':'TAM',\\\n",
    "                            'oti':'TEN','was':'WAS'}},inplace=True)\n",
    "        data[key].dropna(thresh=3,inplace=True)\n",
    "\n",
    "    nfl_injury = pd.concat(data.values(),ignore_index=True)\n",
    "    nfl_injury.to_csv(f'NFL_{Start_Year}_{End_Year}_Injuryreport.csv',index=False)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_Year = 2017\n",
    "End_Year = 2019\n",
    "Start = 2017\n",
    "End = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_data_scrape(Start,End,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_snap_scrape(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# injury_reports_scrape(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_stats_scape(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_snap_stats_clean(Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'injury_reports_clean(Start,End)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1 = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\nflteam_data_1_{Start_Year}_{End_Year}.csv')\n",
    "team2 = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\snapcounts\\nflteam_data_2_{Start_Year}_{End_Year}.csv')\n",
    "team1.dropna(thresh=10,inplace=True)\n",
    "team2.dropna(thresh=10,inplace=True)\n",
    "team1.drop('LTime',axis=1,inplace=True)\n",
    "team2.drop(['LTime'],axis=1,inplace=True)\n",
    "test1 = team1.columns\n",
    "test2 = team2.columns\n",
    "team1.rename(columns={'Tm':'Team','Unnamed: 5':'Away_Home','PF':'Points_For','PA':'Points_Against','PC':'Points_Comb',\\\n",
    "                     'vs. Line':'Vs_Line','Cmp':'Pass_Cmp','Att':'Pass_Att','Cmp%':'Pass_Cmp%','Yds':'Pass_Yds',\\\n",
    "                      'TD':'Pass_TD','Int':'Pass_Int','Sk':'Sack','Yds.1':'Sack_Yds','Rate':'QB_Rate',\\\n",
    "                      'Att.1':'Rush_Att','Yds.2':'Rush_Yds','Y/A':'Rush_Y/A','TD.1':'Rush_TD','Tot':'Tot_Yds',\\\n",
    "                      'Ply':'O_Play#','Y/P':'O_Y/P','DPly':'D_Play#','DY/P':'D_Y/P','TO':'Tot_TO','ToP':'O_ToP',\\\n",
    "                      'Time.1':'Game_Dur','Yds.3':'Pen_Yds','OppPen':'Opp_Pen','OppYds':'Opp_Pen_Yds',\\\n",
    "                      'CombPen':'Comb_Pen','CombPenYds':'Comb_Pen_Yds','1stD':'1st_Downs','Rsh':'1st_by_Rsh',\\\n",
    "                      'Pass':'1st_by_Pass','Pen.1':'1st_by_Pen','3DATT':'3rd_Down_Att','3DConv':'3rd_Down_Conv',\\\n",
    "                      '3D%':'3rd_Down%','4DAtt':'4th_Down_Att','4DConv':'4th_Down_Conv','4D%':'4th_Down%',\\\n",
    "                      'TD.2':'Tot_TD','XPA':'XP_Att','XPM':'XP_Made','FGA':'FG_Att','FGM':'FG_Made','2PA':'2Pt_Att',\\\n",
    "                      '2PM':'2Pt_Made','Pnt':'Times_Punted','Yds.4':'Punt_Yds','Y/P.1':'Punt_Yds_Avg'},inplace=True)\n",
    "team2.rename(columns={'Tm':'Team','Unnamed: 5':'Away_Home','TD':'Opp_Tot_TD','XPA':'Opp_XP_Att','XPM':'Opp_XP_Made',\\\n",
    "                      'Att':'Opp_FG_Att','Md':'Opp_FG_Made','Sfty':'Opp_Sfty','Cmp':'Opp_Pass_Cmp','Att.1':'Opp_Pass_Att',\\\n",
    "                      'Cmp%':'Opp_Pass_Cmp%','Yds':'Opp_Pass_Yds','TD.1':'Opp_Pass_TD','Int':'Opp_Pass_Int','Sk':'Opp_Sk',\\\n",
    "                      'Yds.1':'Opp_Sk_Yds','Rate':'Opp_QB_Rate','Att.2':'Opp_Rush_Att','Yds.2':'Opp_Rush_Yds',\\\n",
    "                      'Y/A':'Opp_Rush_Y/A','TD.2':'Opp_Rush_TD','Tot':'Opp_Tot_Yds','TO':'Opp_Tot_TO',\\\n",
    "                      '1stDOpp':'Opp_1st_Downs','Rush':'Opp_1st_by_Rsh','Pass':'Opp_1st_by_Pass','Pen':'Opp_1st_by_Pen',\\\n",
    "                      'Opp3DATT':'Opp_3rd_Down_Att','Opp3DConv':'Opp_3rd_Down_Conv','Opp3D%':'Opp_3rd_Down%',\\\n",
    "                      'Opp4DAtt':'Opp_4th_Down_Att','Opp4DConv':'Opp_4th_Down_Conv','Opp4D%':'Opp_4th_Down%',\\\n",
    "                      'Rush.1':'Margin_Rush','Pass.1':'Margin_Pass','Tot.1':'Margin_TotYds','TO.1':'TO_TD',\\\n",
    "                      'KR':'KR_TD','PR':'PR_TD','IR':'Int_TD','FR':'Fmb_TD','OR':'OtherRet_TD',\\\n",
    "                      'RetTD':'All_Ret_TD','Q1':'Mar_Thru_Q1','Q2':'Mar_Thru_Q2','Q3':'Mar_Thru_Q3',\\\n",
    "                      'Q1.1':'Score_Diff_Q1','Q2.1':'Score_Diff_Q2','Q3.1':'Score_Diff_Q3',\\\n",
    "                      'Q4':'Score_Diff_Q4','1stHalf':'Score_Diff_1stHalf','2ndHalf':'Score_Diff_2ndHalf'},inplace=True)\n",
    "\n",
    "team = pd.merge(left=team1,right=team2,\\\n",
    "                 how='outer',\\\n",
    "                 on=['Team','Year','Date','Time','Away_Home','Opp','Week','G#','Day','Result','OT'])\n",
    "\n",
    "cols = ['Points_For','Points_Against',\\\n",
    "        'PD','Points_Comb','Spread','Vs_Line','Over/Under','OU Result','Pass_Cmp','Pass_Att','Pass_Cmp%','Pass_Yds',\\\n",
    "        'Pass_TD','Pass_Int','Sack','Sack_Yds','QB_Rate','Rush_Att','Rush_Yds','Rush_Y/A','Rush_TD','Tot_Yds','O_Play#',\\\n",
    "        'O_Y/P','D_Play#','D_Y/P','Tot_TO','O_ToP','Game_Dur','Pen','Pen_Yds','Opp_Pen','Opp_Pen_Yds','Comb_Pen','Comb_Pen_Yds',\\\n",
    "        '1st_Downs','1st_by_Rsh','1st_by_Pass','1st_by_Pen','3DAtt','3rd_Down_Conv','3rd_Down%','4th_Down_Att','4th_Down_Conv',\\\n",
    "        '4th_Down%','Tot_TD','XP_Att','XP_Made','FG_Att','FG_Made','2Pt_Att','2Pt_Made','Sfty','Times_Punted','Punt_Yds',\\\n",
    "        'Punt_Yds_Avg','Opp_Tot_TD','Opp_XP_Att','Opp_XP_Made','Opp_FG_Att','Opp_FG_Made','Opp_Sfty','Opp_Pass_Cmp',\\\n",
    "        'Opp_Pass_Att','Opp_Pass_Cmp%','Opp_Pass_Yds','Opp_Pass_TD','Opp_Pass_Int','Opp_Sk','Opp_Sk_Yds','Opp_QB_Rate',\\\n",
    "        'Opp_Rush_Att','Opp_Rush_Yds','Opp_Rush_Y/A','Opp_Rush_TD','Opp_Tot_Yds','Opp_Tot_TO','Opp_1st_Downs','Opp_1st_by_Rsh',\\\n",
    "        'Opp_1st_by_Pass','Opp_1st_by_Pen','Opp3DAtt','Opp_3rd_Down_Conv','Opp_3rd_Down%','Opp_4th_Down_Att',\\\n",
    "        'Opp_4th_Down_Conv','Opp_4th_Down%','Margin_Rush','Margin_Pass','Margin_TotYds','TO_TD','KR_TD','PR_TD','Int_TD',\\\n",
    "        'Fmb_TD','OtherRet_TD','All_Ret_TD','Mar_Thru_Q1','Mar_Thru_Q2','Mar_Thru_Q3','Score_Diff_Q1','Score_Diff_Q2',\\\n",
    "        'Score_Diff_Q3','Score_Diff_Q4','Score_Diff_1stHalf','Score_Diff_2ndHalf']\n",
    "\n",
    "team.replace({'Away_Home':{'@':'Away',None:'Home'}},inplace=True)\n",
    "team[cols] = team[cols].fillna(value=0)\n",
    "team['Date'] = pd.to_datetime(team['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "team['Game_Dur'] = team['Game_Dur']+':00'\n",
    "team['O_ToP'] = '00:'+team['O_ToP']\n",
    "team['Game_Dur'] = pd.to_timedelta(team['Game_Dur'],errors='coerce')\n",
    "team['Game_Dur'] = team['Game_Dur'].dt.total_seconds()\n",
    "team['O_ToP'] = pd.to_timedelta(team['O_ToP'],errors='coerce')\n",
    "team['O_ToP'] = team['O_ToP'].dt.total_seconds()\n",
    "team.insert(loc=9,column='Month',value=team['Date'].dt.month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
