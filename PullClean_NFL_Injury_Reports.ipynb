{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def webscrap_nfl_injury_reports(Start_Year,End_Year):\n",
    "    try:\n",
    "        #list of the NFL teams for url purposes\n",
    "        teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "                 'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "        years = []\n",
    "\n",
    "        #list of years to pull data for url purposes\n",
    "        for yr in range(Start_Year,End_Year+1):\n",
    "            years.append(yr)\n",
    "\n",
    "        #starting points to iterate through\n",
    "        team = 0\n",
    "        year = 0\n",
    "        dfname = []\n",
    "\n",
    "        while team < 2:\n",
    "\n",
    "            dfname.append(f'{teams[team]}_{years[year]}_injuryreport')\n",
    "\n",
    "            #url for web scraping\n",
    "            url = f'https://www.pro-football-reference.com/teams/{teams[team]}/{years[year]}_injuries.htm'\n",
    "\n",
    "            #opening website\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "            #finding table\n",
    "            table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "            table_rows = table.find_all('tr')\n",
    "\n",
    "            #scraping the data\n",
    "            final_data = []\n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all(['th','td'])\n",
    "                row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "                final_data.append(row)\n",
    "\n",
    "            #creatingdataframe to save\n",
    "            dfdata = final_data[1:]\n",
    "            data_body = [[dfdata[j][i] for j in range(len(dfdata))] for i in range(len(dfdata[0]))]\n",
    "            data = {key: pd.DataFrame(data_body,final_data[0]).T for key in dfname}\n",
    "\n",
    "            key = f'{teams[team]}_{years[year]}_injuryreport'\n",
    "\n",
    "            data[key].insert(loc=1,column='Team',value=teams[team],allow_duplicates=True)\n",
    "            data[key].insert(loc=2,column='Year',value=years[year],allow_duplicates=True)\n",
    "            data[f'{teams[team]}_{years[year]}_injuryreport'].to_csv(f'{teams[team]}_{years[year]}_injuryreport.csv',index=True)\n",
    "\n",
    "            # update of location of web scrape\n",
    "            print(teams[team],years[year])\n",
    "\n",
    "            #advancing through url\n",
    "            if year < len(years)-1:\n",
    "                year += 1\n",
    "\n",
    "            else:\n",
    "                year = 0\n",
    "                team += 1\n",
    "    \n",
    "    except:\n",
    "        print('Error')\n",
    "\n",
    "\n",
    "# def clean_nfl_injury_reports(Start_Year,End_Year):\n",
    "#     try:\n",
    "#         dfname = []\n",
    "#         teams = ['crd', 'atl']#, 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "#                          #'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "#         for year in range(Start_Year,End_Year+1):\n",
    "#             for team in teams:\n",
    "#                 Start_Year = Start_Year\n",
    "#                 dfname.append(f'{team}_{year}_injuryreport')\n",
    "#                 data = {key: pd.read_csv(f'{team}_{year}_injuryreport.csv') for key in dfname}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         for key in data:\n",
    "#             data[key].drop(['TTL','AVG'],axis=1,inplace=True)\n",
    "#             data[key] = pd.melt(data[key],id_vars=['Player','Pos','Team','Year'],\\\n",
    "#                       value_vars=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17'])\n",
    "#             data[key].rename(columns={'variable':'Week','value':'Snaps'},inplace=True)\n",
    "#             data[key].replace({'Team':\\\n",
    "#                       {'GB':'GNB','JAC':'JAX','KC':'KAN','NE':'NWE','NO':'NOR','SF':\\\n",
    "#                        'SFO','TB':'TAM','Multi':None,'LV':'OAK',},'Snaps':{'bye':None}},inplace=True)\n",
    "#             data[key].dropna(thresh=3,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         player = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "#         team = pd.read_csv('nflteamsnaps.csv')\n",
    "#         team.drop(['LTime','Y/P'],axis=1,inplace=True)\n",
    "#         team.rename(columns={'Unnamed: 6':'Away_Home','Time.1':'ToG'},inplace=True)\n",
    "#         team['Date'] = pd.to_datetime(team['Date'])\n",
    "#         team['ToG'] = pd.to_datetime(team['ToG'])\n",
    "#         team['Away_Home'].fillna(value='Home',inplace=True)\n",
    "#         team['Away_Home'].replace('@','Away',inplace=True)\n",
    "#         team['Away_Home'].replace('nan','Home',inplace=True)\n",
    "#         team['TO'].fillna(value=0,inplace=True)\n",
    "#         team.insert(loc=2,column='Month',value=team['Date'].dt.month)\n",
    "#         df_bridge = team['ToP'].str.split(\":\",expand=True)\n",
    "#         team['ToP'] = (df_bridge[0].astype(int)*60)+df_bridge[1].astype(int)\n",
    "#         team['Week'] = team['Week'].astype(str)\n",
    "\n",
    "#         player_snaps = pd.merge(left=player,right=team,how='outer',left_on=['Team','Year','Week'],right_on=['Tm','Year','Week'])\n",
    "\n",
    "#         print(player)\n",
    "#         print(team)\n",
    "#         print(player_snaps)\n",
    "#         player_snaps.to_csv(f'player_snaps_{Start_Year}_{End_Year}.csv',index=False)\n",
    "        \n",
    "#         if year < len(years)-1:\n",
    "#                 year += 1\n",
    "#         else:\n",
    "#             year = 0\n",
    "#             team += 1\n",
    "#     except:\n",
    "#         print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crd 2017\n",
      "crd 2018\n",
      "crd 2019\n",
      "atl 2017\n",
      "atl 2018\n",
      "atl 2019\n"
     ]
    }
   ],
   "source": [
    "webscrap_nfl_injury_reports(2017,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Player , Team, Year, 09/08vs. MIN, 09/15vs. PHI, 09/22vs. IND, 09/29vs. TEN, 10/06vs. HOU, 10/13vs. ARI, 10/20vs. LAR, 10/27vs. SEA, 11/10vs. NOR, 11/17vs. CAR, 11/24vs. TAM, 11/28vs. NOR, 12/08vs. CAR, 12/15vs. SFO, 12/22vs. JAX, 12/29vs. TAM]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "Start_Year = 2017\n",
    "End_Year = 2019\n",
    "\n",
    "dfname = []\n",
    "teams = ['crd', 'atl']#, 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "                 #'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "for year in range(Start_Year,End_Year+1):\n",
    "    for team in teams:\n",
    "        Start_Year = Start_Year\n",
    "        dfname.append(f'{team}_{year}_injuryreport')\n",
    "        data = {key: pd.read_csv(f'{team}_{year}_injuryreport.csv') for key in dfname}\n",
    "\n",
    "print(data['crd_2017_injuryreport'][:0])\n",
    "\n",
    "# for key in data:\n",
    "#     data[key] = pd.melt(colname[key],id_vars=colname[key][1:3],value_vars=colname[key][4:])\n",
    "#     data[key].rename(columns={'PlayerÂ':'Player','variable':'Date','value':'Status'},inplace=True)\n",
    "#     data[key].drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#     #data[key].replace({'Team':\\\n",
    "#               #{'GB':'GNB','JAC':'JAX','KC':'KAN','NE':'NWE','NO':'NOR','SF':\\\n",
    "#                #'SFO','TB':'TAM','Multi':None,'LV':'OAK',},'Snaps':{'bye':None}},inplace=True)\n",
    "#     #data[key].dropna(thresh=3,inplace=True)\n",
    "\n",
    "# print(data)\n",
    "\n",
    "# player = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "# team = pd.read_csv('nflteamsnaps.csv')\n",
    "# team.drop(['LTime','Y/P'],axis=1,inplace=True)\n",
    "# team.rename(columns={'Unnamed: 6':'Away_Home','Time.1':'ToG'},inplace=True)\n",
    "# team['Date'] = pd.to_datetime(team['Date'])\n",
    "# team['ToG'] = pd.to_datetime(team['ToG'])\n",
    "# team['Away_Home'].fillna(value='Home',inplace=True)\n",
    "# team['Away_Home'].replace('@','Away',inplace=True)\n",
    "# team['Away_Home'].replace('nan','Home',inplace=True)\n",
    "# team['TO'].fillna(value=0,inplace=True)\n",
    "# team.insert(loc=2,column='Month',value=team['Date'].dt.month)\n",
    "# df_bridge = team['ToP'].str.split(\":\",expand=True)\n",
    "# team['ToP'] = (df_bridge[0].astype(int)*60)+df_bridge[1].astype(int)\n",
    "# team['Week'] = team['Week'].astype(str)\n",
    "\n",
    "# player_snaps = pd.merge(left=player,right=team,how='outer',left_on=['Team','Year','Week'],right_on=['Tm','Year','Week'])\n",
    "\n",
    "# print(player)\n",
    "# print(team)\n",
    "# print(player_snaps)\n",
    "# player_snaps.to_csv(f'player_snaps_{Start_Year}_{End_Year}.csv',index=False)\n",
    "\n",
    "# if year < len(years)-1:\n",
    "#         year += 1\n",
    "# else:\n",
    "#     year = 0\n",
    "#     team += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
