{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy Sidelines \n",
    "### Code to Scrape, Clean, Save, and Analyze NFL injury, player stats, team stats, and player snaps data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash, requests, os, time\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Section 1 Scrape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.1 - Team Weekly Stats Scraped From www.stathead.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data_scrape(Season,url):\n",
    "    # scrape weekly team stats from www.stathead.com\n",
    "    start = time.time()\n",
    "    # page to start scrape at\n",
    "    page = 0\n",
    "    # login payload information pulled from a .env file\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    \n",
    "    # lots of team data, could not fit into one pull from stathead, needed to use 2 different url's, this allows the function to iterate through both\n",
    "    if url == 1:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=points&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=E&game_num_min=0&year_min={Season}&cstat[1]=all_td_team&ccomp[1]=gt&cval[1]=0&cstat[2]=third_down_att&ccomp[2]=gt&cval[2]=0&cstat[3]=vegas_line&ccomp[3]=gt&cval[3]=-50&cstat[4]=penalties&ccomp[4]=gt&cval[4]=0&cstat[5]=rush_att&ccomp[5]=gt&cval[5]=0&cstat[6]=tot_yds&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=pass_cmp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url == 2:\n",
    "        stat_url = 'https://stathead.com/football/tgl_finder.cgi?request=1&temperature_gtlt=lt&game_num_max=99&week_num_max=99&order_by=all_td_opp&match=game&year_max={Season}&order_by_asc=0&week_num_min=0&game_type=R&game_num_min=0&year_min={Season}&cstat[1]=tot_yds_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_yds_diff&ccomp[2]=gt&cval[2]=-500&cstat[3]=score_diff_thru_1&ccomp[3]=gt&cval[3]=-500&cstat[4]=rush_att_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=kick_ret_td_tgl&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp_opp&ccomp[6]=gt&cval[6]=0&cstat[7]=first_down_opp&ccomp[7]=gt&cval[7]=0&cstat[8]=score_diff_1_qtr&ccomp[8]=gt&cval[8]=-500&cstat[9]=third_down_att_opp&ccomp[9]=gt&cval[9]=0&offset={page}'\n",
    "    elif url != 1 or 2:\n",
    "        print(\"Please select 1 or 2.\")\n",
    "    \n",
    "    # open logged in session for scraping\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        # beginning the scrape and stopping the scrape when page number reaches 100k\n",
    "        try:\n",
    "\n",
    "            while page < 100000:\n",
    "                \n",
    "                # pulling the website and scraping it\n",
    "                website = session.get(stat_url.format(Season=Season,page=page)).text\n",
    "                soup = BeautifulSoup(website, 'html')\n",
    "                table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "                \n",
    "                # pull headers and rows out of the data\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "                \n",
    "                # final location for complete data\n",
    "                final_data = []\n",
    "                \n",
    "                # create row for each line of data in table\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "                \n",
    "                # create the dataframe in panadas excluding the blank row and matching headers with the rows\n",
    "                df = pd.DataFrame(final_data[1:], columns=table_headers[12:])\n",
    "                \n",
    "                # writting dataframe to csv, continuous appending just incase the function fails, data will be saved\n",
    "                if url == 1:\n",
    "                    df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\nflteam_data_1_{Season}_raw.csv',mode='a',index=False)\n",
    "                else:\n",
    "                    df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\nflteam_data_2_{Season}_raw.csv',mode='a',index=False)\n",
    "                \n",
    "                # progress through the websites\n",
    "                page += 100\n",
    "            \n",
    "        except:\n",
    "            # notifying the scrape has completed.\n",
    "            end = time.time()\n",
    "            print(f'Done: Team {Season}, {url}, {page}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.2 - Player Weekly Snap Data Scraped From www.fantasypros.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_snap_scrape(Season):\n",
    "\n",
    "    # starting the weekly player snap scrape\n",
    "    start = time.time()\n",
    "    \n",
    "    # two different sides of the ball, editing the url to allow to grab both sides\n",
    "    sides = ['','defense.php']\n",
    "    # create the range of weeks needed for the scrape\n",
    "    weeks = []\n",
    "    for wk in range(1,18):\n",
    "        weeks.append(wk)\n",
    "    \n",
    "    ENDPOINT = \"https://www.fantasypros.com/nfl/reports/snap-count-analysis/{side}?year={Season}&week={week}&snaps=0&range=week\"\n",
    "    \n",
    "    # create final storage place for data\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    # scrape pages for both sides of the ball\n",
    "    for side in sides:\n",
    "\n",
    "        # scrape pages for each week of the season\n",
    "        for week in weeks:\n",
    "\n",
    "            # opening the webpage and storing data into lists then to dataframe\n",
    "            res = requests.get(ENDPOINT.format(Season=Season,side=side,week=week))\n",
    "\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "            table = soup.find('table', {'id': 'data'})\n",
    "            table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "            table_rows = table.find_all('tr')\n",
    "\n",
    "            final_data = []\n",
    "\n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all('td')\n",
    "                row = [tr.text for tr in td]\n",
    "                final_data.append(row)\n",
    "            \n",
    "            # create dataframe from current data\n",
    "            df = pd.DataFrame(final_data[1:], columns=table_headers)\n",
    "            df['Season'] = Season\n",
    "            df['Week'] = week\n",
    "            \n",
    "            # add current data to final dataframe\n",
    "            final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # write dataframe to csv\n",
    "    final_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\snapcounts_{Season}_raw.csv',index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Snaps {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.3 - NFL Weekly Injury Reports Scraped From www.pro-football-reference.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injury_reports_scrape(Season):\n",
    "\n",
    "    # scrape weekly nfl injury reports\n",
    "    start = time.time()\n",
    "    # list of team abbreviations from pro-football-reference for url purposes\n",
    "    teams = ['crd', 'atl', 'rav', 'buf', 'car', 'chi', 'cin', 'cle', 'dal', 'den', 'det', 'gnb','htx','clt','jax','kan',\n",
    "             'sdg','ram','mia','min','nor','nwe','nyg','nyj','rai','phi','pit','sea','sfo','tam','oti','was']\n",
    "\n",
    "    ENDPOINT = 'https://www.pro-football-reference.com/teams/{team}/{Season}_injuries.htm'\n",
    "    \n",
    "    # creating final place to store data\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    # scrape pages for all teams\n",
    "    for team in teams:\n",
    "        \n",
    "        # open webpage and scrape contents into lists, then to a dataframe\n",
    "        res = requests.get(ENDPOINT.format(Season=Season,team=team))\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "        table = soup.find('table', attrs={'class': 'sortable', 'id': 'team_injuries'})\n",
    "        table_rows = table.find_all('tr')\n",
    "\n",
    "        final_data = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all(['th','td'])\n",
    "            row = [tr['data-tip'] if tr.has_attr(\"data-tip\") else tr.text for tr in td]\n",
    "            final_data.append(row)\n",
    "\n",
    "        dfdata = final_data[1:]\n",
    "        data_body = [[dfdata[j][i] for j in range(len(dfdata))] for i in range(len(dfdata[0]))]\n",
    "\n",
    "        df = pd.DataFrame(data_body,final_data[0]).T\n",
    "        # adding team and season columns for identification\n",
    "        df.insert(loc=1,column='Team',value=team)\n",
    "        df.insert(loc=2,column='Season',value=Season)\n",
    "\n",
    "        # combine current data with final dataframe\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # rename column\n",
    "    final_df.rename(columns={'PlayerÂ ':'Player'},inplace=True)\n",
    "    \n",
    "    # write final data to csv file\n",
    "    final_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\nfl_injuryreport_{Season}_raw.csv',index=False)\n",
    "    end = time.time()\n",
    "    print(f'Done: Injury Reports {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 1.4 - Player Weekly Stats Scraped From www.stathead.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_stats_scape(Season):\n",
    "    \n",
    "    # scrape weekly player stats from statehead.com\n",
    "    start = time.time()\n",
    "    # page/location for monitoring progress and continuing through pages\n",
    "    page = 0\n",
    "    location = 2000\n",
    "    \n",
    "    # stathead login info pulled from .env file\n",
    "    stat_login_url = \"https://stathead.com/users/login.cgi\"\n",
    "    stat_user_name = os.environ.get('statheadusername')\n",
    "    stat_password = os.environ.get('statheadpassword')\n",
    "    stat_payload = {\n",
    "        'username': stat_user_name,\n",
    "        'password': stat_password\n",
    "    }\n",
    "    stat_url = \"https://stathead.com/football/pgl_finder.cgi?request=1&game_num_max=99&week_num_max=99&order_by=all_td&season_start=1&qb_gwd=0&order_by_asc=0&qb_comeback=0&week_num_min=0&game_num_min=0&year_min={Season}&match=game&year_max={Season}&season_end=-1&age_min=0&game_type=R&age_max=99&positions[]=qb&positions[]=rb&positions[]=wr&positions[]=te&positions[]=e&positions[]=t&positions[]=g&positions[]=c&positions[]=ol&positions[]=dt&positions[]=de&positions[]=dl&positions[]=ilb&positions[]=olb&positions[]=lb&positions[]=cb&positions[]=s&positions[]=db&positions[]=k&positions[]=p&cstat[1]=punt_ret&ccomp[1]=gt&cval[1]=0&cstat[2]=sacks&ccomp[2]=gt&cval[2]=0&cstat[3]=fumbles&ccomp[3]=gt&cval[3]=0&cstat[4]=rush_att&ccomp[4]=gt&cval[4]=0&cstat[5]=pass_defended&ccomp[5]=gt&cval[5]=0&cstat[6]=pass_cmp&ccomp[6]=gt&cval[6]=0&cstat[7]=targets&ccomp[7]=gt&cval[7]=0&cstat[8]=kick_ret&ccomp[8]=gt&cval[8]=0&offset={page}\"\n",
    "\n",
    "    # logging into session to begin scrape\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        s = session.post(stat_login_url, data=stat_payload)\n",
    "        \n",
    "        # begin scrape, once it fails, stop scrape\n",
    "        try:\n",
    "\n",
    "            # scrape webpages up to 100k contents\n",
    "            while page < 100000:\n",
    "\n",
    "                # opening webpage and storing data into list then dataframe\n",
    "                website = session.get(stat_url.format(Season=Season,page=page)).text\n",
    "                soup = BeautifulSoup(website, 'html')\n",
    "                table = soup.find('table', attrs={'class': 'sortable', 'id': 'results'})\n",
    "\n",
    "                table_headers = [header.text for header in table.find('thead').find_all('th')]\n",
    "                table_rows = table.find_all('tr')\n",
    "\n",
    "                final_data = []\n",
    "\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    final_data.append(row)\n",
    "\n",
    "                df = pd.DataFrame(final_data[1:], columns=table_headers[11:])\n",
    "                df.rename(columns={'Year':'Season'},inplace=True)\n",
    "                \n",
    "                # appendings csv with data from current dataframe to prevent loss if code fails or connection drops\n",
    "                df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\player_stats_{Season}_raw.csv',mode='a',index=False)\n",
    "\n",
    "                # continue through webpages and update where the location of the scrape is\n",
    "                if page > location:\n",
    "                    print('Player Stats on page:',(page-100))\n",
    "                    location += 2000\n",
    "\n",
    "                page += 100\n",
    "                \n",
    "        except:\n",
    "            # end of the scrape notificaiton\n",
    "            end = time.time()\n",
    "            print(f'Done: Stats {Season}, {page}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Section 2 - Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.1 - Clean Team Weekly Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data_clean(Season):\n",
    "    # begin cleaning team data\n",
    "    start = time.time()\n",
    "    # opening both csv files into dataframes for cleaning and combining\n",
    "    team1 = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\nflteam_data_1_{Season}_raw.csv')\n",
    "    team2 = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\nflteam_data_2_{Season}_raw.csv')\n",
    "    # drop all blank rows\n",
    "    team1.dropna(thresh=10,inplace=True)\n",
    "    team2.dropna(thresh=10,inplace=True)\n",
    "    # drop useless columns\n",
    "    team1.drop('LTime',axis=1,inplace=True)\n",
    "    team2.drop(['LTime'],axis=1,inplace=True)\n",
    "    # rename columns\n",
    "    team1.rename(columns={'Tm':'Team','Unnamed: 5':'Away_Home','PF':'Points_For','PA':'Points_Against','PC':'Points_Comb',\\\n",
    "                         'vs. Line':'Vs_Line','Cmp':'TPass_Cmp','Att':'TPass_Att','Cmp%':'TPass_Cmp%','Yds':'TPass_Yds',\\\n",
    "                          'TD':'TPass_TD','Int':'TPass_Int','Sk':'TSack','Yds.1':'TSack_Yds','Rate':'TQB_Rate',\\\n",
    "                          'Att.1':'TRush_Att','Yds.2':'TRush_Yds','Y/A':'TRush_Y/A','TD.1':'TRush_TD','Tot':'TTot_Yds',\\\n",
    "                          'Ply':'TO_Play#','Y/P':'TO_Y/P','DPly':'TD_Play#','DY/P':'TD_Y/P','TO':'TTot_TO','ToP':'TO_ToP',\\\n",
    "                          'Time.1':'TGame_Dur','Yds.3':'TPen_Yds','OppPen':'TOpp_Pen','OppYds':'TOpp_Pen_Yds',\\\n",
    "                          'CombPen':'TComb_Pen','CombPenYds':'TComb_Pen_Yds','1stD':'T1st_Downs','Rsh':'T1st_by_Rsh',\\\n",
    "                          'Pass':'T1st_by_Pass','Pen.1':'T1st_by_Pen','3DAtt':'T3rd_Down_Att','3DConv':'T3rd_Down_Conv',\\\n",
    "                          '3D%':'T3rd_Down%','4DAtt':'T4th_Down_Att','4DConv':'T4th_Down_Conv','4D%':'T4th_Down%',\\\n",
    "                          'TD.2':'TTot_TD','XPA':'TXP_Att','XPM':'TXP_Made','FGA':'TFG_Att','FGM':'TFG_Made','2PA':'T2Pt_Att',\\\n",
    "                          '2PM':'T2Pt_Made','Sfty':'TSfty','Pnt':'TTimes_Punted','Yds.4':'TPunt_Yds','Y/P.1':'TPunt_Yds_Avg','Year':'Season'},inplace=True)\n",
    "    team2.rename(columns={'Tm':'Team','Unnamed: 5':'Away_Home','TD':'TOpp_Tot_TD','XPA':'TOpp_XP_Att','XPM':'TOpp_XP_Made',\\\n",
    "                          'Att':'TOpp_FG_Att','Md':'TOpp_FG_Made','Sfty':'TOpp_Sfty','Cmp':'TOpp_Pass_Cmp','Att.1':'TOpp_Pass_Att',\\\n",
    "                          'Cmp%':'TOpp_Pass_Cmp%','Yds':'TOpp_Pass_Yds','TD.1':'TOpp_Pass_TD','Int':'TOpp_Pass_Int','Sk':'TOpp_Sk',\\\n",
    "                          'Yds.1':'TOpp_Sk_Yds','Rate':'TOpp_QB_Rate','Att.2':'TOpp_Rush_Att','Yds.2':'TOpp_Rush_Yds',\\\n",
    "                          'Y/A':'TOpp_Rush_Y/A','TD.2':'TOpp_Rush_TD','Tot':'TOpp_Tot_Yds','TO':'TOpp_Tot_TO',\\\n",
    "                          '1stDOpp':'TOpp_1st_Downs','Rush':'TOpp_1st_by_Rsh','Pass':'TOpp_1st_by_Pass','Pen':'TOpp_1st_by_Pen',\\\n",
    "                          'Opp3DAtt':'TOpp_3rd_Down_Att','Opp3DConv':'TOpp_3rd_Down_Conv','Opp3D%':'TOpp_3rd_Down%',\\\n",
    "                          'Opp4DAtt':'TOpp_4th_Down_Att','Opp4DConv':'TOpp_4th_Down_Conv','Opp4D%':'TOpp_4th_Down%',\\\n",
    "                          'Rush.1':'TMargin_Rush','Pass.1':'TMargin_Pass','Tot.1':'TMargin_TotYds','TO.1':'TTO_TD',\\\n",
    "                          'KR':'TKR_TD','PR':'TPR_TD','IR':'TInt_TD','FR':'TFmb_TD','OR':'TOtherRet_TD',\\\n",
    "                          'RetTD':'TAll_Ret_TD','Q1':'TMar_Thru_Q1','Q2':'TMar_Thru_Q2','Q3':'TMar_Thru_Q3',\\\n",
    "                          'Q1.1':'TScore_Diff_Q1','Q2.1':'TScore_Diff_Q2','Q3.1':'TScore_Diff_Q3',\\\n",
    "                          'Q4':'TScore_Diff_Q4','1stHalf':'TScore_Diff_1stHalf','2ndHalf':'TScore_Diff_2ndHalf','Year':'Season'},inplace=True)\n",
    "    \n",
    "    # merge the two dataframes based on team, season, date, time, away/home, opp, week, g#, dat, result, and ot\n",
    "    team = pd.merge(left=team1,right=team2,\\\n",
    "                     how='outer',\\\n",
    "                     on=['Team','Season','Date','Time','Away_Home','Opp','Week','G#','Day','Result','OT'])\n",
    "    team.set_index('Team',inplace=True)\n",
    "    # drop all rows that have headers in them\n",
    "    team.drop('Tm',inplace=True)\n",
    "    team.reset_index(inplace=True)\n",
    "\n",
    "    # create a list of all the column names in the team dataframe\n",
    "    team_cols = []\n",
    "    \n",
    "    for col in team.columns:\n",
    "        team_cols.append(col)\n",
    "\n",
    "    # cleaning the away/home column to show away or home instead of @\n",
    "    team.replace({'Away_Home':{'@':'Away',None:'Home'}},inplace=True)\n",
    "    team[team_cols[11:]] = team[team_cols[11:]].fillna(value=0)\n",
    "    # change datatype of the columns\n",
    "    team[['TPass_Cmp','TPass_Att','T3rd_Down_Att','T3rd_Down_Conv','T4th_Down_Att',\\\n",
    "          'T4th_Down_Conv','TOpp_Pass_Cmp','TOpp_Pass_Att','TOpp_3rd_Down_Att',\\\n",
    "          'TOpp_3rd_Down_Conv','TOpp_4th_Down_Att','TOpp_4th_Down_Conv']] = team[['TPass_Cmp','TPass_Att','T3rd_Down_Att',\\\n",
    "                                                                                  'T3rd_Down_Conv','T4th_Down_Att',\\\n",
    "                                                                                  'T4th_Down_Conv','TOpp_Pass_Cmp',\\\n",
    "                                                                                  'TOpp_Pass_Att','TOpp_3rd_Down_Att',\\\n",
    "                                                                                  'TOpp_3rd_Down_Conv','TOpp_4th_Down_Att',\\\n",
    "                                                                                  'TOpp_4th_Down_Conv']].astype(float)\n",
    "    # calculating the columns to show apporpriate values\n",
    "    team['TPass_Cmp%'] = team['TPass_Cmp']/team['TPass_Att']\n",
    "    team['T3rd_Down%'] = team['T3rd_Down_Conv']/team['T3rd_Down_Att']\n",
    "    team['T4th_Down%'] = team['T4th_Down_Conv']/team['T4th_Down_Att']\n",
    "    team['TOpp_Pass_Cmp%'] = team['TOpp_Pass_Cmp']/team['TOpp_Pass_Att']\n",
    "    team['TOpp_3rd_Down%'] = team['TOpp_3rd_Down_Conv']/team['TOpp_3rd_Down_Att']\n",
    "    team['TOpp_4th_Down%'] = team['TOpp_4th_Down_Conv']/team['TOpp_4th_Down_Att']\n",
    "    # changing dates/time to datetime values\n",
    "    team['Date'] = pd.to_datetime(team['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "    team['TGame_Dur'] = team['TGame_Dur']+':00'\n",
    "    team['TO_ToP'] = '00:'+team['TO_ToP']\n",
    "    team['TGame_Dur'] = pd.to_timedelta(team['TGame_Dur'],errors='coerce')\n",
    "    team['TGame_Dur'] = team['TGame_Dur'].dt.total_seconds()\n",
    "    team['TO_ToP'] = pd.to_timedelta(team['TO_ToP'],errors='coerce')\n",
    "    team['TO_ToP'] = team['TO_ToP'].dt.total_seconds()\n",
    "    #changing datatypes of columns\n",
    "    team[team_cols[11:16]] = team[team_cols[11:16]].astype(float)\n",
    "    team[team_cols[17]] = team[team_cols[17]].astype(float)\n",
    "    team[team_cols[19:]] = team[team_cols[19:]].astype(float)\n",
    "    # adding month column\n",
    "    team.insert(loc=9,column='Month',value=team['Date'].dt.month)\n",
    "    # chaning week datatype and removing any week larger than week 17\n",
    "    team['Week'] = team['Week'].astype(int)\n",
    "    team = team[team['Week']<=17]\n",
    "    team['Week'] = team['Week'].astype(str)\n",
    "    # saving cleaned data to csv\n",
    "    team.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Clean\\nflteam_data_{Season}_clean.csv',index=False)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Team data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.2 - Clean Player Weekly Snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_snaps_clean(Season):\n",
    "    \n",
    "    start = time.time()\n",
    "    snaps = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\snapcounts_{Season}_raw.csv')\n",
    "    # drop unnecessary columns\n",
    "    snaps.drop(['Fantasy Pts','Pts/100 Snaps','Rush %','Tgt %','Touch %','Util %','Tackle %','Sack %','QB Hit %','Snaps/Gm'],axis=1,inplace=True)\n",
    "    # change datatypes and calculate columns approriately\n",
    "    snaps['Season'] = snaps['Season'].astype(str)\n",
    "    snaps['Snap %'] = snaps['Snap %']/100\n",
    "    snaps['Week'] = snaps['Week'].astype(str)\n",
    "    # change team names to be consistent\n",
    "    snaps.replace({'Team':{'FA':'','GB':'GNB','JAC':'JAX','KC':'KAN','NE':'NWE','NO':'NOR','SF':'SFO','TB':'TAM'}},inplace=True)\n",
    "    # cleaning player names to be consistent\n",
    "    snaps['Player'] = snaps['Player']+' '\n",
    "    snaps.replace({'Player':{' Jr. ':'',' Jr ':'',' Sr. ':'',' Sr ':'',' III ':'',' II ':'',' IV ':'',' V ':''}},regex=True,inplace=True)\n",
    "    snaps['Player'] = snaps['Player'].str.strip(' ')\n",
    "    # save cleaned snap data\n",
    "    snaps.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Clean\\snapcounts_{Season}_clean.csv',index=False)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Snap data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.3 - Clean NFL Weekly Injury Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfl_injury_clean(Season):\n",
    "    \n",
    "    start = time.time()\n",
    "    injury = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\nfl_injuryreport_{Season}_raw.csv',low_memory=False)\n",
    "    # separate file to help calculate week numbers\n",
    "    nfl_weeks = pd.read_csv(r'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\NFL Week Dates.csv')\n",
    "    # melt columns to break each week into a separate row\n",
    "    injury = pd.melt(injury,id_vars=['Player','Team','Season'],var_name='Date', value_name='Status')\n",
    "    # create 2 columns based off a header\n",
    "    injury[['Date','Opp']] = injury.Date.str.split('vs. ',expand=True)\n",
    "    injury[['Month','Day']] = injury.Date.str.split('/',expand=True)\n",
    "    injury[['Status','Injury']] = injury.Status.str.split(\":\",expand=True)\n",
    "    # drop all values that are empty after the melting/splitting\n",
    "    injury.dropna(axis=0,subset=['Status','Injury'],how='all',inplace=True)\n",
    "    # change datatypes for cleaning\n",
    "    injury[['Season','Month','Day']] = injury[['Season','Month','Day']].astype(int)\n",
    "    # combine datat to create a date for the game and change to datetime value\n",
    "    injury['Date'] = injury['Date']+'/'+(np.where(injury['Month']<=2,injury['Season']+1,injury['Season'])).astype(str)\n",
    "    injury['Date'] = pd.to_datetime(injury['Date'])\n",
    "    nfl_weeks['Week'] = nfl_weeks['Week'].astype(str)\n",
    "    # create function to calculate what nfl week the date occured\n",
    "    def pre_thu(d):\n",
    "        days_behind = 3 - d.weekday()\n",
    "        if days_behind > 0:\n",
    "            days_behind -= 7\n",
    "        return d + dt.timedelta(days_behind)\n",
    "    injury['week_start_nfl'] = injury['Date'].apply(pre_thu)\n",
    "    nfl_weeks['Start Date'] = pd.to_datetime(nfl_weeks['Start Date'])\n",
    "    # merge injury data with the nfl week data to pull the weeks into current datagrame\n",
    "    injury = pd.merge(left=injury,right=nfl_weeks,how='left',left_on='week_start_nfl',right_on='Start Date')\n",
    "    # update team names for consistency\n",
    "    injury.replace({'Team':\\\n",
    "                       {'crd':'ARI', 'atl':'ATL', 'rav':'BAL', 'buf':'BUF', 'car':'CAR', 'chi':'CHI', 'cin':'CIN',\\\n",
    "                        'cle':'CLE', 'dal':'DAL', 'den':'DEN', 'det':'DET', 'gnb':'GNB','htx':'HOU','clt':'IND',\\\n",
    "                        'jax':'JAX','kan':'KAN','sdg':'LAC','ram':'LAR','mia':'MIA','min':'MIN','nor':'NOR','nwe':'NWE',\\\n",
    "                        'nyg':'NYG','nyj':'NYJ','rai':'OAK','phi':'PHI','pit':'PIT','sea':'SEA','sfo':'SFO','tam':'TAM',\\\n",
    "                        'oti':'TEN','was':'WAS'}},inplace=True)\n",
    "    # clean injury data for easier management and searching of data\n",
    "    injury['Injury'] = injury['Injury'].str.strip(' ')\n",
    "    injury.replace({'Injury':{'right':'','left':'','Right':'','Left':'','Biceps':'Bicep',\\\n",
    "                              'Triceps':'Tricep','Ankles':'Ankle','hip':'Hip','Hips':'Hip','Knees':'Knee',\\\n",
    "                              'Virus':'Illness','Triceps':'Tricep','Oblique':'Abdomen',\\\n",
    "                              'NotInjuryRelated':'Not Injury Related','MedicalIllness':'Illness',\\\n",
    "                              'LowerLeg':'Lower Leg','CoreMuscle':'Abdomen','Abdominal':'Abdomen'}},\\\n",
    "                               regex=True,inplace=True)\n",
    "    # clean player names for consistency\n",
    "    injury['Player'] = injury['Player']+' '\n",
    "    injury.replace({'Player':{' Jr. ':'',' Jr ':'',' Sr. ':'',' Sr ':'',' III ':'',' II ':'',' IV ':'',' V ':''}},regex=True,inplace=True)\n",
    "    injury['Player'] = injury['Player'].str.strip(' ')\n",
    "    # drop unnecessary columns\n",
    "    injury.drop(['Month','Day','week_start_nfl','Start Date'],axis=1,inplace=True)\n",
    "    # drop all empty week values\n",
    "    injury.dropna(subset=['Week'],inplace=True)\n",
    "    # change datatypes\n",
    "    injury[['Player','Week','Season']] = injury[['Player','Week','Season']].astype(str)\n",
    "    # reorganize columns\n",
    "    injury = injury[['Player','Team','Opp','Date','Season','Week','Status','Injury']]\n",
    "    # add two empty columns for data input\n",
    "    injury[['Specific_Inj','Side']] = None\n",
    "    injury.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Clean\\injury_report_{Season}_clean.csv',index=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Injury data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 2.4 - Clean Player Weekly Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_stats_clean(Season):\n",
    "    \n",
    "    start = time.time()\n",
    "    stats = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Raw\\player_stats_{Season}_raw.csv')\n",
    "    # drop empty rows\n",
    "    stats.dropna(how='all',inplace=True)\n",
    "    # drop all rows with header information\n",
    "    stats.drop(stats[stats['Player'] == 'Player'].index, inplace = True)\n",
    "    stats.drop('Lg',axis=1,inplace=True)\n",
    "    # rename columns for better management\n",
    "    stats.rename(columns={'Tm':'Team','Unnamed: 6':'Away_Home','Cmp':'IPass_Cmp','Att':'IPass_Att','Cmp%':'IPass_Cmp%','Yds':'IPass_Yds',\\\n",
    "                 'TD':'IPass_TD','Int':'IPass_Int','Rate':'IQB_Rate','Sk':'I_Sk','Yds.1':'ISk_Yds','Y/A':'IPass_Y/A',\\\n",
    "                 'AY/A':'IPass_AdjY/A','Att.1':'IRush_Att','Yds.2':'IRush_Yds','Y/A.1':'IRush_Y/A','TD.1':'IRush_TD',\\\n",
    "                 'Tgt':'IRec_Tgt','Rec':'IRec_Rec','Yds.3':'IRec_Yds','Y/R':'IRec_Y/R','TD.2':'IRec_TD','Ctch%':'IRec_Ctch%',\\\n",
    "                 'Y/Tgt':'IRec_Y/Tgt','XPM':'IXP_Made','XPA':'IXP_Att','XP%':'IXP%','FGM':'IFG_Made','FGA':'IFG_Att',\\\n",
    "                 'FG%':'IFG%','2PM':'I2pt_Made','Sfty':'ISfty','TD.3':'ITot_TD','Pts':'ITot_Pts','Rt':'IKR_Rt','Yds.4':'IKR_Yds',\\\n",
    "                 'Y/Rt':'IKR_Y/Rt','TD.4':'IKR_TD','Ret':'IPR_Rt','Yds.5':'IPR_Yds','Y/R.1':'IPR_Y/Rt','TD.5':'IPR_TD',\\\n",
    "                 'Sk.1':'ITack_Sk','Solo':'ITack_Solo','Ast':'ITack_Ast','Comb':'ITack_Tot','TFL':'ITack_TFL',\\\n",
    "                 'QBHits':'ITack_QBHits','Int.1':'IDef_Int','Yds.6':'IDef_IntYds','TD.6':'IDef_IntTD','PD':'IDef_PD',\\\n",
    "                 'Fmb':'IFmb_Fmb','FL':'IFmb_Lost','FF':'IFmb_Forced','FR':'IFmb_Recov','Yds.7':'IFmb_Yds','TD.7':'IFmb_TD'},\\\n",
    "                 inplace=True)\n",
    "    # create a list of the columns\n",
    "    stats_cols = []\n",
    "    for col in stats.columns:\n",
    "        stats_cols.append(col)\n",
    "    # replace blanks and @ with home and away\n",
    "    stats.replace({'Away_Home':{'@':'Away',None:'Home'}},inplace=True)\n",
    "    # change datatypes and calculate data appropriately\n",
    "    stats[['IPass_Cmp','IPass_Att','IRec_Rec','IRec_Tgt','IXP_Made','IXP_Att','IFG_Made','IFG_Att']] = stats[['IPass_Cmp','IPass_Att','IRec_Rec','IRec_Tgt','IXP_Made','IXP_Att','IFG_Made','IFG_Att']].astype(float)\n",
    "    stats['IPass_Cmp%'] = stats['IPass_Cmp']/stats['IPass_Att']\n",
    "    stats['IRec_Ctch%'] = stats['IRec_Rec']/stats['IRec_Tgt']\n",
    "    stats['IXP%'] = stats['IXP_Made']/stats['IXP_Att']\n",
    "    stats['IFG%'] = stats['IFG_Made']/stats['IFG_Att']\n",
    "    # change datatypes\n",
    "    stats[stats_cols[11:]] = stats[stats_cols[11:]].astype(float)\n",
    "    stats[stats_cols[11:]] = stats[stats_cols[11:]].fillna(value=0)\n",
    "    stats['Date'] = pd.to_datetime(stats['Date'],errors='coerce',format='%Y-%m-%d')\n",
    "    # insert a column for season\n",
    "    stats.insert(loc=4,column='Season',value=Season)\n",
    "    stats['Player'] = stats['Player'].astype(str)\n",
    "    stats['Week'] = stats['Week'].astype(str)\n",
    "    stats['Season'] = stats['Season'].astype(str)\n",
    "    # clean player names for consistency\n",
    "    stats['Player'] = stats['Player']+' '\n",
    "    stats.replace({'Player':{' Jr. ':'',' Jr ':'',' Sr. ':'',' Sr ':'',' III ':'',' II ':'',' IV ':'',' V ':''}},regex=True,inplace=True)\n",
    "    stats['Player'] = stats['Player'].str.strip(' ')\n",
    "    stats.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Clean\\player_stats_{Season}_clean.csv',index=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Player stats data cleaned. {Season}',f'Time to complete: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Grouped Run Fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 3.1 - Scrape All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scrape(Start,End):\n",
    "    # run all web scrape functions at once for multiple season\n",
    "    start = time.time()\n",
    "    season = []\n",
    "    for yr in range(Start,End+1):\n",
    "        season.append(yr)\n",
    "    for s in season:\n",
    "        team_data_scrape(s,1)\n",
    "        team_data_scrape(s,2)\n",
    "        player_snap_scrape(s)\n",
    "        injury_reports_scrape(s)\n",
    "        player_stats_scape(s)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Scrape complete.',f'Time to complete season {s}: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### 3.2 - Clean All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clean(Start,End):\n",
    "    # run all data clean functions at once for multiple seasons\n",
    "    start = time.time()\n",
    "    season = []\n",
    "    for yr in range(Start,End+1):\n",
    "        season.append(yr)\n",
    "    for s in season:\n",
    "        team_data_clean(s)\n",
    "        player_snaps_clean(s)\n",
    "        nfl_injury_clean(s)\n",
    "        player_stats_clean(s)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Data clean complete.',f'Time to compelte season {s}: {end-start}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_scrape(2016,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_clean(2016,2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Custom Fantasy Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Accept Custom Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect custom scoring format for each league\n",
    "def custom_scoring():\n",
    "    while True:\n",
    "        print(\"Please enter in custom scoring format: \")\n",
    "        lg_pass_yds = 1/float(input('Passing yards per 1 point league value: '))\n",
    "        lg_pass_tds = float(input('Passing touchdowns league value: '))\n",
    "        lg_int = float(input('Offensive interceptions league value: '))\n",
    "        lg_yds = 1/float(input('Non-passing yards per 1 point league value: '))\n",
    "        lg_tds = float(input('Non-passing touchdowns league value: '))\n",
    "        lg_rec_rec = float(input('Receptions league value: '))\n",
    "        lg_2pt = float(input('2-point conversion league value: '))\n",
    "        lg_fmb = float(input('Fumbles lost league value: '))\n",
    "        lg_kicker_xp = float(input('Point after attempt made league value: '))\n",
    "        lg_fg_yds = 1/float(input('Field goal yards per 1 point league value: '))\n",
    "        lg_def_sk = float(input('Defensive sack league value: '))\n",
    "        lg_def_int = float(input('Defensive interceptions league value: '))\n",
    "        lg_def_fmb = float(input('Defensive fumble recovery league value: '))\n",
    "        lg_def_td = float(input('Defensive touchdown league value: '))\n",
    "        lg_def_sfty = float(input('Defensive saftey league value: '))\n",
    "        lg_def_block = float(input('Defensive blocked kick league value: '))\n",
    "        lg_spec_td = float(input('Special teams touchdown league value: '))\n",
    "        lg_def_0pts = float(input('Defensive allowed points 0 league value: '))\n",
    "        lg_def_6pts = float(input('Defensive allowed points 1-6 league value: '))\n",
    "        lg_def_13pts = float(input('Defensive allowed points 7-13 league value: '))\n",
    "        lg_def_20pts = float(input('Defensive allowed points 14-20 league value: '))\n",
    "        lg_def_27pts = float(input('Defensive allowed points 21-27 league value: '))\n",
    "        lg_def_34pts = float(input('Defensive allowed points 28-34 league value: '))\n",
    "        lg_def_35pts = float(input('Defensive allowed points 35+ league value: '))\n",
    "        lg_spec_xpreturn = float(input('Special teams extra point returned league value: '))\n",
    "        idp_solo_tck = float(input('IDP points per solo tackle league value: '))\n",
    "        idp_ass_tck = float(input('IDP points per assisted tackle league value: '))\n",
    "        idp_tck_loss = float(input('IDP points per tackle for loss league value: '))\n",
    "        idp_sack = float(input('IDP points per sack league value: '))\n",
    "        idp_int = float(input('IDP points per interception league value: '))\n",
    "        idp_ffmb  = float(input('IDP points per forced fumble league value: '))\n",
    "        idp_fmbrec = float(input('IDP points per fumble recovery league value: '))\n",
    "        idp_td = float(input('IDP points per defensive touchdown league value: '))\n",
    "        idp_sfty = float(input('IDP points per defensive saftey league value: '))\n",
    "        idp_passd = float(input('IDP points per pass defended league value: '))\n",
    "\n",
    "        cust_scoring = {'lg_pass_yds':lg_pass_yds,\n",
    "                        'lg_pass_tds':lg_pass_tds,\n",
    "                        'lg_int':lg_int,\n",
    "                        'lg_yds':lg_yds,\n",
    "                        'lg_tds':lg_tds,\n",
    "                        'lg_rec_rec':lg_rec_rec,\n",
    "                        'lg_2pt':lg_2pt,\n",
    "                        'lg_fmb':lg_fmb,\n",
    "                        'lg_kicker_xp':lg_kicker_xp,\n",
    "                        'lg_fg_yds':lg_fg_yds,\n",
    "                        'lg_def_sk':lg_def_sk,\n",
    "                        'lg_def_int':lg_def_int,\n",
    "                        'lg_def_fmb':lg_def_fmb,\n",
    "                        'lg_def_td':lg_def_td,\n",
    "                        'lg_def_sfty':lg_def_sfty,\n",
    "                        'lg_def_block':lg_def_block,\n",
    "                        'lg_spec_td':lg_spec_td,\n",
    "                        'lg_def_0pts':lg_def_0pts,\n",
    "                        'lg_def_6pts':lg_def_6pts,\n",
    "                        'lg_def_13pts':lg_def_13pts,\n",
    "                        'lg_def_20pts':lg_def_20pts,\n",
    "                        'lg_def_27pts':lg_def_27pts,\n",
    "                        'lg_def_34pts':lg_def_34pts,\n",
    "                        'lg_def_35pts':lg_def_35pts,\n",
    "                        'lg_spec_xpreturn':lg_spec_xpreturn,\n",
    "                        'idp_solo_tck':idp_solo_tck,\n",
    "                        'idp_ass_tck':idp_ass_tck,\n",
    "                        'idp_tck_loss':idp_tck_loss,\n",
    "                        'idp_sack':idp_sack,\n",
    "                        'idp_int':idp_int,\n",
    "                        'idp_ffmb':idp_ffmb,\n",
    "                        'idp_fmbrec':idp_fmbrec,\n",
    "                        'idp_td':idp_td,\n",
    "                        'idp_sfty':idp_sfty,\n",
    "                        'idp_passd':idp_passd\n",
    "                       }\n",
    "        # save custom scoring to a text file for use later\n",
    "        file = open(r'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\Custom_Scoring.txt','w')\n",
    "        for k in cust_scoring:\n",
    "            file.write(f'{k},{cust_scoring[k]}\\n')\n",
    "        file.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter in custom scoring format: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Passing yards per 1 point league value:  25\n",
      "Passing touchdowns league value:  4\n",
      "Offensive interceptions league value:  -2\n",
      "Non-passing yards per 1 point league value:  10\n",
      "Non-passing touchdowns league value:  6\n",
      "Receptions league value:  0.5\n",
      "2-point conversion league value:  2\n",
      "Fumbles lost league value:  -2\n",
      "Point after attempt made league value:  1\n",
      "Field goal yards per 1 point league value:  10\n",
      "Defensive sack league value:  1\n",
      "Defensive interceptions league value:  2\n",
      "Defensive fumble recovery league value:  2\n",
      "Defensive touchdown league value:  6\n",
      "Defensive saftey league value:  2\n",
      "Defensive blocked kick league value:  2\n",
      "Special teams touchdown league value:  6\n",
      "Defensive allowed points 0 league value:  10\n",
      "Defensive allowed points 1-6 league value:  7\n",
      "Defensive allowed points 7-13 league value:  4\n",
      "Defensive allowed points 14-20 league value:  1\n",
      "Defensive allowed points 21-27 league value:  0\n",
      "Defensive allowed points 28-34 league value:  -1\n",
      "Defensive allowed points 35+ league value:  -3\n",
      "Special teams extra point returned league value:  2\n",
      "IDP points per solo tackle league value:  1\n",
      "IDP points per assisted tackle league value:  0.5\n",
      "IDP points per tackle for loss league value:  1\n",
      "IDP points per sack league value:  2\n",
      "IDP points per interception league value:  2\n",
      "IDP points per forced fumble league value:  2\n",
      "IDP points per fumble recovery league value:  2\n",
      "IDP points per defensive touchdown league value:  6\n",
      "IDP points per defensive saftey league value:  2\n",
      "IDP points per pass defended league value:  2\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'OneDrive/Podcasting/Fantasy Sidelines/Injury Data Python/Data_Collect_Clean/nfl_data/Custom_Scoring.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2930a5902773>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcustom_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-54a79816b957>\u001b[0m in \u001b[0;36mcustom_scoring\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m                        }\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# save custom scoring to a text file for use later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'OneDrive/Podcasting/Fantasy Sidelines/Injury Data Python/Data_Collect_Clean/nfl_data/Custom_Scoring.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcust_scoring\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{k},{cust_scoring[k]}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OneDrive/Podcasting/Fantasy Sidelines/Injury Data Python/Data_Collect_Clean/nfl_data/Custom_Scoring.txt'"
     ]
    }
   ],
   "source": [
    "custom_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull custom scoring format from txt file\n",
    "file = open('Custom_Scoring.txt','r')\n",
    "score_format = {}\n",
    "for line in file:\n",
    "    column = line.split(',')\n",
    "    scor_format[column[0]] = float(column[1])\n",
    "file.close()\n",
    "\n",
    "# combine data\n",
    "def combine(Season):\n",
    "    start = time.time()\n",
    "    stats_snaps = pd.merge(left=stats,right=snaps,how='outer',on=['Player','Season','Week'])\n",
    "    stats_snaps['Team_x'] = stats_snaps['Team_x'].fillna(stats_snaps['Team_y'])\n",
    "    stats_snaps['Pos_x'] = stats_snaps['Pos_y'].fillna(stats_snaps['Pos_x'])\n",
    "    stats_snaps.drop(['Team_y','Pos_y'],axis=1,inplace=True)\n",
    "    stats_snaps.rename(columns={'Team_x':'Team','Pos_x':'Pos'},inplace=True)\n",
    "\n",
    "    print(f'Stats and snaps data merged. {Season}')\n",
    "\n",
    "    stats_snaps_injury = pd.merge(left=stats_snaps,right=injury,how='outer',on=['Player','Season','Week'])\n",
    "    stats_snaps_injury['Date_x'] = stats_snaps_injury['Date_x'].fillna(stats_snaps_injury['Date_y'])\n",
    "    stats_snaps_injury['Team_x'] = stats_snaps_injury['Team_x'].fillna(stats_snaps_injury['Team_y'])\n",
    "    stats_snaps_injury['Opp_x'] = stats_snaps_injury['Opp_x'].fillna(stats_snaps_injury['Opp_y'])\n",
    "    stats_snaps_injury.drop(['Date_y','Team_y','Opp_y'],axis=1,inplace=True)\n",
    "    stats_snaps_injury.rename(columns={'Date_x':'Date','Team_x':'Team','Opp_x':'Opp'},inplace=True)\n",
    "\n",
    "    print(f'Stats, snaps, and injuries merged. {Season}')\n",
    "    \n",
    "    combined = pd.merge(left=stats_snaps_injury,right=team,how='outer',on=['Team','Season','Week'])\n",
    "    combined['Date_x'] = combined['Date_x'].fillna(combined['Date_y'])\n",
    "    combined['Away_Home_x'] = combined['Away_Home_x'].fillna(combined['Away_Home_y'])\n",
    "    combined['Opp_x'] = combined['Opp_x'].fillna(combined['Opp_y'])\n",
    "    combined['G#_x'] = combined['G#_x'].fillna(combined['G#_y'])\n",
    "    combined['Day_x'] = combined['Day_x'].fillna(combined['Day_y'])\n",
    "    combined['Result_x'] = combined['Result_x'].fillna(combined['Result_y'])\n",
    "    combined.drop(['Date_y','Away_Home_y','Opp_y','G#_y','Day_y','Result_y'],axis=1,inplace=True)\n",
    "    combined.rename(columns={'Date_x':'Date','Away_Home_x':'Away_Home','Opp_x':'Opp','G#_x':'G#','Day_x':'Day','Result_x':'Result'},inplace=True)\n",
    "    \n",
    "    combined.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\clean\\nfl_{Season}.csv',index=False)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Data clean done for season {Season}, time to complete: ',end-start)\n",
    "\n",
    "# first date injury occured\n",
    "def injury_report_firstinj(Season):\n",
    "    injury_df = pd.read_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\nfl_{Season}.csv')\n",
    "    injury_df.drop_duplicates(['Player','Season','Injury'],inplace=True)\n",
    "    injury_df.to_csv(rf'C:\\Users\\cudde\\OneDrive\\Podcasting\\Fantasy Sidelines\\Injury Data Python\\Data_Collect_Clean\\nfl_data\\nfl_injury_initial_{Season}.csv',index=False)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fantasy total from custom score\n",
    "nfl_data['Fantasy_Pts_Total'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose how to filter the NFL data\n",
    "filter_pass_att = int(input('Filter by how many pass attempts minimum? '))\n",
    "filter_rush_att = int(input('Filter by how many rush attempts minimum? '))\n",
    "filter_rec_tgt = int(input('Filter by how many receiving targets minimum? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out pass attempts or rush attempts or receiving targets lower than X\n",
    "nfl_data = nfl_data.query(f'Pass_Att >= {filter_pass_att} or Rush_Att >= {filter_rush_att} or Rec_Tgt>= {filter_rec_tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making some values integeres as they cannot be floats\n",
    "# nfl_data = nfl_data.astype({'Game_Num':'int64','Week':'int64','Pass_Att':'int64','Pass_TD':'int64',\\\n",
    "#                             'Pass_Yds':'int64','Pass_INT':'int64','Rush_Att':'int64','Rush_Yds':'int64',\\\n",
    "#                             'Rush_TD':'int64','Rec_Tgt':'int64','Rec_Rec':'int64','Rec_Yds':'int64',\\\n",
    "#                             'Rec_TD':'int64','K_Ret_TD':'int64','P_Ret_TD':'int64','Fmb_L':'int64','Fmb_TD':'int64'})\n",
    "\n",
    "# grouping data for plotting\n",
    "\n",
    "# try multi (hierarchical) indexing instead of groupby to help split the weeks?\n",
    "\n",
    "nfl_week_std = nfl_data.groupby('Week')['Fantasy_Pts_Total'].std()\n",
    "nfl_pos_std = nfl_data.groupby('Pos')['Fantasy_Pts_Total'].std()\n",
    "nfl_player_std = nfl_data.groupby('Player')['Fantasy_Pts_Total'].std()\n",
    "nfl_day_std = nfl_data.groupby('Day')['Fantasy_Pts_Total'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping data for plotting\n",
    "\n",
    "# try multi (hierarchical) indexing instead of groupby to help split the weeks?\n",
    "\n",
    "nfl_pos_week_avg = nfl_data.groupby(['Pos','Week'])['Fantasy_Pts_Total'].mean()\n",
    "nfl_player_week_avg = nfl_data.groupby(['Player','Week'])['Fantasy_Pts_Total'].mean()\n",
    "nfl_player_day_avg = nfl_data.groupby(['Player','Day'])['Fantasy_Pts_Total'].mean()\n",
    "nfl_player_opp_avg = nfl_data.groupby(['Player','Opp'])['Fantasy_Pts_Total'].mean()\n",
    "nfl_pos_week_std = nfl_data.groupby(['Pos','Week'])['Fantasy_Pts_Total'].std()\n",
    "nfl_player_week_std = nfl_data.groupby(['Player','Week'])['Fantasy_Pts_Total'].std()\n",
    "nfl_player_day_std = nfl_data.groupby(['Player','Day'])['Fantasy_Pts_Total'].std()\n",
    "nfl_player_opp_std = nfl_data.groupby(['Player','Opp'])['Fantasy_Pts_Total'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
