{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://lucahammer.com/2019/11/05/collecting-old-tweets-with-the-twitter-premium-api-and-python/\n",
    "# https://twitterdev.github.io/search-tweets-python/\n",
    "# https://github.com/twitterdev/search-tweets-python/tree/master/examples\n",
    "'''\n",
    "Example script to collect old Tweets with the Twitter Premium Search API\n",
    "Article: https://lucahammer.com/?p=350\n",
    "\n",
    "To use this script, change the constants (UPPERCASE variables) to your needs,\n",
    "and run it. For example in your CLI by executing: \"python premiumapi.py\".\n",
    "\n",
    "Find your app credentials here: https://developer.twitter.com/en/apps\n",
    "Find your dev environment label here: https://developer.twitter.com/en/account/environments\n",
    "'''\n",
    "# Variables used to be able to pull the data needed from twitter\n",
    "API_SCOPE = 'fullarchive'\n",
    "DEV_ENVIRONMENT_LABEL = 'injuries'\n",
    "SEARCH_QUERY = 'Christian McCaffrey Shoulder'\n",
    "RESULTS_PER_CALL = 100  # 100 for sandbox, 500 for paid tiers\n",
    "TO_DATE = '2020-11-15' # format YYYY-MM-DD HH:MM (hour and minutes optional)\n",
    "FROM_DATE = '2020-11-1'  # format YYYY-MM-DD HH:MM (hour and minutes optional)\n",
    "\n",
    "# max number of tweets to collect\n",
    "MAX_RESULTS = 3\n",
    "\n",
    "# create csv\n",
    "FILENAME = 'twitter_nfl_injury.csv'  # Where the Tweets should be saved\n",
    "\n",
    "# Script prints an update to the CLI every time it collected another X Tweets\n",
    "PRINT_AFTER_X = 1\n",
    "\n",
    "# create a yaml document to hold credentials\n",
    "# import yaml\n",
    "# config = dict(\n",
    "#     search_tweets_api=dict(\n",
    "#         account_type='premium',\n",
    "#         endpoint=f\"https://api.twitter.com/1.1/tweets/search/{API_SCOPE}/{DEV_ENVIRONMENT_LABEL}.json\",\n",
    "#         consumer_key=API_KEY,\n",
    "#         consumer_secret=API_SECRET_KEY\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# with open('twitter_keys.yaml', 'w') as config_file:\n",
    "#     yaml.dump(config, config_file, default_flow_style=False)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from searchtweets import load_credentials, gen_rule_payload, ResultStream\n",
    "\n",
    "# opening the communication\n",
    "premium_search_args = load_credentials(\"twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "\n",
    "# creating the payload for search\n",
    "rule = gen_rule_payload(SEARCH_QUERY,\n",
    "                        results_per_call=RESULTS_PER_CALL,\n",
    "                        from_date=FROM_DATE,\n",
    "                        to_date=TO_DATE\n",
    "                        )\n",
    "\n",
    "#returning the results from the payload and stops at max_results\n",
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=MAX_RESULTS,\n",
    "                  **premium_search_args)\n",
    "\n",
    "# working properly, need to filter out retweets by using retweeted_status to remove retweets would be easiest in a dataframe\n",
    "# need to format it for a pandas DF and write into csv instead of writing the full dictionary\n",
    "# find headers within the data so that the differing size rows get split up properly\n",
    "\n",
    "# loop to open FILENAME as f and write to f the value of tweet which would be results_per_call\n",
    "with open(FILENAME, 'a', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for tweet in rs.stream():\n",
    "        n += 1\n",
    "        # once n reaches value of print_after_x function discontinues\n",
    "        if n % PRINT_AFTER_X == 0:\n",
    "            print('{0}: {1}'.format(str(n), tweet['created_at']))\n",
    "        print(tweet)\n",
    "        #df = pd.DataFrame.from_dict(tweet)\n",
    "        # writing tweets to file\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
